{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../original_images/psvrt.py:3: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-fce1b3103be7>\", line 1, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2, os, math, time, sys\n",
    "from datetime import timedelta\n",
    "from sklearn.utils import shuffle\n",
    "sys.path.append('../../original_images')\n",
    "from gen_data_batch import generate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Configuration\n",
    "\"\"\"\n",
    "Data Configurations/Paths\n",
    "\"\"\"\n",
    "img_dir_patch=\"./SD/predicted_patches\"\n",
    "img_dir_orig = \"../../original_images/SD\"\n",
    "\n",
    "modelsd_test = 'SD/modelsd_test.ckpt'\n",
    "# img_type = \"original\"\n",
    "img_type = \"patch\"\n",
    "\n",
    "##\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size3 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# Convolutional Layer 4.\n",
    "filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 256             # Number of neurons in fully-connected layer.\n",
    "\n",
    "# Number of colour channels for the images: 3 channel for RGB.\n",
    "num_channels = 3\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (60, 60, num_channels)\n",
    "item_size = (5,5)\n",
    "# Number of classes, one class for same and one for different image\n",
    "num_classes = 2\n",
    "nitems = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../original_images/psvrt.py:253: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  running_orientation += np.arctan(y / x)\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10800)\n",
      "(64, 60, 60)\n",
      "(64, 60, 60)\n",
      "(64, 60, 60)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_batch_size = 64\n",
    "train_data, mask_labels, left_mask, right_mask, labels = generate_batch(train_batch_size, img_shape, item_size, nitems)\n",
    "print(train_data.shape)\n",
    "print(mask_labels.shape)\n",
    "print(left_mask.shape)\n",
    "print(right_mask.shape)\n",
    "see_output(np.reshape(train_data, [64, 60,60,3])[:1, :,:,:])\n",
    "see_output_grey(mask_labels[:1, :,:])\n",
    "see_output_grey(left_mask[:1, :, :])\n",
    "see_output_grey(right_mask[:1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(img_dir, img_type=\"patch\"):\n",
    "        list_of_imgs = []\n",
    "        list_same_diff = []\n",
    "        for img_no in os.listdir(img_dir):\n",
    "            img_no_path = os.path.join(img_dir, img_no)\n",
    "            for img_label in os.listdir(img_no_path):\n",
    "                    \n",
    "                list_same_diff.append(int(img_label))\n",
    "                img_lbl_path = os.path.join(img_no_path, img_label)\n",
    "#                 print(img_lbl_path)\n",
    "                if img_type == \"original\":\n",
    "                    img_lbl_path = img_lbl_path + \"/img/\"\n",
    "                    \n",
    "                if img_type == \"patch\":\n",
    "                    for img in os.listdir(img_lbl_path):\n",
    "#                         if img == \"labels\":\n",
    "                        img_path = os.path.join(img_lbl_path, img)\n",
    "#                             img_path = img_path + \"/merged_patch.png\"\n",
    "                        list_of_imgs.append(img_path)\n",
    "                else:    \n",
    "                    for img in os.listdir(img_lbl_path):\n",
    "                        img_path = os.path.join(img_lbl_path, img)\n",
    "                        list_of_imgs.append(img_path)\n",
    "#         print(list_of_imgs)\n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "        data_same_diff = np.array(list_same_diff)\n",
    "\n",
    "        return data_imgs, data_same_diff\n",
    "    \n",
    "def get_batch_images(data, same_diff, type_img = \"patch\"):\n",
    "        list_of_imgs = []\n",
    "        list_of_same_diff = []\n",
    "        for img, img_type in zip(data, same_diff):\n",
    "            orig_img = cv2.imread(img)\n",
    "            #only first image as a label\n",
    "            if orig_img is None:\n",
    "                    print (\"Unable to read image{}\".format(img))\n",
    "                    continue\n",
    "            \n",
    "            if type_img == \"original\":\n",
    "                flattened_img = orig_img.flatten()\n",
    "                list_of_imgs.append(np.asarray(flattened_img, dtype=np.float32))\n",
    "                \n",
    "                if img_type == 1: #0 is same and 1 is different\n",
    "                    list_of_same_diff.append([0,1])\n",
    "                else:\n",
    "                    list_of_same_diff.append([1,0])\n",
    "            else:            \n",
    "                if orig_img.shape == (4, 2, 3):\n",
    "                    flattened_img = orig_img.flatten()\n",
    "                    list_of_imgs.append(np.asarray(flattened_img, dtype=np.float32))\n",
    "\n",
    "                    if img_type == 1: #0 is same and 1 is different\n",
    "                        list_of_same_diff.append([0,1])\n",
    "                    else:\n",
    "                        list_of_same_diff.append([1,0])\n",
    "        \n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "        data_img_type = np.array(list_of_same_diff)\n",
    "        \n",
    "        return data_imgs, data_img_type\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "def patch_next_batch(num, data, lft_lbls, rght_lbls, img_tlbl, img_key):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_orig = data[0, :]\n",
    "    data_pred = data[1, :]\n",
    "    data_orig_shuffle = [data[0, i] for i in idx]\n",
    "    data_pred_shuffle = [data[1, i] for i in idx]\n",
    "    lft_labels = [lft_lbls[ i] for i in idx]\n",
    "    rght_labels = [rght_lbls[ i] for i in idx]\n",
    "    img_tlbl = [img_tlbl[ i] for i in idx]\n",
    "    img_key = [img_key[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_orig_shuffle), np.asarray(data_pred_shuffle), np.asarray(lft_labels), np.asarray(rght_labels), np.asarray(img_tlbl), np.asarray(img_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape, layer_name):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape), name=layer_name+'_W')\n",
    "\n",
    "def new_bias(length, layer_name):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]), name=layer_name+'_b')\n",
    "\n",
    "def new_conv_layer(input,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   name_scope,\n",
    "                   layer_name='',\n",
    "                   use_pooling=True):\n",
    "\n",
    "    with tf.name_scope(name_scope):\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "        weights = new_weights(shape, layer_name)\n",
    "        biases = new_bias(num_filters, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.nn.conv2d(input=input, filter=weights, strides=[1,1,1,1], padding='SAME'), biases, name=layer_name)\n",
    "\n",
    "        if use_pooling:\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, 3, 3, 1],\n",
    "                                   strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME', name=layer_name+'_max')\n",
    "        layer = tf.nn.relu(layer, name=layer_name+'_activation')\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,\n",
    "                num_inputs,\n",
    "                num_outputs,\n",
    "                name_scope,\n",
    "                layer_name='',\n",
    "                use_relu=True):\n",
    "    \n",
    "    with tf.name_scope(name_scope):\n",
    "        weights = new_weights([num_inputs, num_outputs], layer_name)\n",
    "        biases = new_bias(num_outputs, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.matmul(input, weights),biases,name=layer_name)\n",
    "    #     layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer, layer_name+'_activation')\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def normalise(tensor):\n",
    "    return tf.div(\n",
    "   tf.subtract(\n",
    "      tensor, \n",
    "      tf.reduce_min(tensor)\n",
    "   ), \n",
    "   tf.subtract(\n",
    "      tf.reduce_max(tensor), \n",
    "      tf.reduce_min(tensor)\n",
    "   )\n",
    ")\n",
    "\n",
    "def normalized(arr):\n",
    "    return (arr - np.min(arr))/(np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_graph layer configurations\n",
    "m_filter_size0 = 16          # Convolution filters are 4 x 4 pixels.\n",
    "m_num_filters0 = 16         # There are 16 of these filters.\n",
    "\n",
    "m_filter_size1 = 8          # Convolution filters are 4 x 4 pixels.\n",
    "m_num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "m_filter_size2 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters2 = 16         # There are 32 of these filters.\n",
    "\n",
    "m_filter_size3 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters3 = 4         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "m_filter_size4 = 4          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters4 = 32         # There are 64 of these filters.\n",
    "\n",
    "m_filter_size5 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters5 = 16         # There are 64 of these filters.\n",
    "\n",
    "\n",
    "# Fully-connected layer.\n",
    "m_fc_size = 2000             # Number of neurons in fully-connected layer.\n",
    "\n",
    "mask_graph = tf.Graph()\n",
    "with mask_graph.as_default():\n",
    "    m_x = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1] * img_shape[2]], name='m_x')\n",
    "    m_x_image = tf.reshape(m_x, [-1, 10, 10, num_channels])\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1]], name='m_y_true')\n",
    "    y_true_cls = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1]], name='m_y_true_cls')    \n",
    "    \n",
    "    layer0_conv0, weights_conv0 = new_conv_layer(input=m_x_image,\n",
    "                                                num_input_channels=num_channels,\n",
    "                                                filter_size=m_filter_size0,\n",
    "                                                num_filters=m_num_filters0,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv1',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "    layer1_conv1, weights_conv1 = new_conv_layer(input=layer0_conv0,\n",
    "                                                num_input_channels=m_num_filters0,\n",
    "                                                filter_size=m_filter_size1,\n",
    "                                                num_filters=m_num_filters1,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv2',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "\n",
    "    layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "                                               num_input_channels=m_num_filters1,\n",
    "                                               filter_size=m_filter_size2,\n",
    "                                               num_filters=m_num_filters2,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv3',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "                                               num_input_channels=m_num_filters2,\n",
    "                                               filter_size=m_filter_size3,\n",
    "                                               num_filters=m_num_filters3,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv4',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "                                               num_input_channels=m_num_filters3,\n",
    "                                               filter_size=m_filter_size4,\n",
    "                                               num_filters=m_num_filters4,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv5',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "\n",
    "    layer5_conv5, weights_conv5 =  new_conv_layer(input=layer4_conv4,\n",
    "                                               num_input_channels=m_num_filters4,\n",
    "                                               filter_size=m_filter_size5,\n",
    "                                               num_filters=m_num_filters5,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv6',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer_flat, num_features = flatten_layer(layer5_conv5)\n",
    "\n",
    "    layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                             num_inputs=num_features,\n",
    "                             num_outputs=m_fc_size,\n",
    "                             name_scope = 'mask',\n",
    "                             layer_name = 'fc1',\n",
    "                             use_relu=True)\n",
    "\n",
    "    layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                             num_inputs=m_fc_size,\n",
    "                             num_outputs=m_fc_size,\n",
    "                             name_scope = 'mask',\n",
    "                             layer_name = 'fc2',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                             num_inputs=m_fc_size,\n",
    "                             num_outputs=m_fc_size,\n",
    "                             name_scope = 'mask',\n",
    "                             layer_name = 'fc3',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "                             num_inputs=m_fc_size,\n",
    "                             num_outputs=img_shape[0] * img_shape[1],\n",
    "                             name_scope = 'mask',\n",
    "                             layer_name = 'fc4',\n",
    "                             use_relu=False)\n",
    "\n",
    "    # drop_out = tf.nn.dropout(layer_fc4, 0.5, name=\"drop_out\")\n",
    "\n",
    "    # y_pred = tf.nn.softmax(layer_fc4, name=\"softmax_output\")\n",
    "    y_pred = layer_fc4\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "    # ## some more performance measures\n",
    "    correct_prediction = tf.equal(y_pred, y_true)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_graph = tf.Graph()\n",
    "with sd_graph.as_default():\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, img_shape[0]*img_shape[1]*num_channels], name='x')\n",
    "    x_image = tf.reshape(x, [-1, img_shape[0], img_shape[1], num_channels])\n",
    "    y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "    y_true_cls = tf.argmax(y_true, axis=1)        \n",
    "\n",
    "\n",
    "    #         layer1_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "    #                                                     num_input_channels=num_channels,\n",
    "    #                                                     filter_size=filter_size1,\n",
    "    #                                                     num_filters=num_filters1,\n",
    "    #                                                     use_pooling=True)\n",
    "\n",
    "    #         layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "    #                                                    num_input_channels=num_filters1,\n",
    "    #                                                    filter_size=filter_size2,\n",
    "    #                                                    num_filters=num_filters2,\n",
    "    #                                                    use_pooling=True)\n",
    "\n",
    "    #         layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "    #                                                    num_input_channels=num_filters2,\n",
    "    #                                                    filter_size=filter_size3,\n",
    "    #                                                    num_filters=num_filters3,\n",
    "    #                                                    use_pooling=True)\n",
    "\n",
    "    #         layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "    #                                                    num_input_channels=num_filters3,\n",
    "    #                                                    filter_size=filter_size4,\n",
    "    #                                                    num_filters=num_filters4,\n",
    "    #                                                    use_pooling=True)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    layer1_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                                num_input_channels=num_channels,\n",
    "                                                filter_size=filter_size1,\n",
    "                                                num_filters=num_filters1,\n",
    "                                                 name_scope = 'sd_graph',\n",
    "                                                 layer_name = 'conv1',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "    layer2_conv2, weights_conv2 = new_conv_layer(input=layer1_conv1,\n",
    "                                                num_input_channels=num_filters1,\n",
    "                                                filter_size=filter_size2,\n",
    "                                                num_filters=num_filters2,\n",
    "                                                 name_scope = 'sd_graph',\n",
    "                                                 layer_name = 'conv2',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "\n",
    "    layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "                                               num_input_channels=num_filters2,\n",
    "                                               filter_size=filter_size3,\n",
    "                                               num_filters=num_filters3,\n",
    "                                                 name_scope = 'sd_graph',\n",
    "                                                 layer_name = 'conv3',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "                                               num_input_channels=num_filters3,\n",
    "                                               filter_size=filter_size4,\n",
    "                                               num_filters=num_filters4,\n",
    "                                                 name_scope = 'sd_graph',\n",
    "                                                 layer_name = 'conv4',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer_flat, num_features = flatten_layer(layer4_conv4)       \n",
    "\n",
    "    layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                     num_inputs=num_features,\n",
    "                     num_outputs=fc_size,\n",
    "                     name_scope = 'sd_graph',\n",
    "                     layer_name = 'fc1',\n",
    "                     use_relu=True)\n",
    "\n",
    "    layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=fc_size,\n",
    "                             name_scope = 'sd_graph',\n",
    "                             layer_name = 'fc2',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=fc_size,\n",
    "                             name_scope = 'sd_graph',\n",
    "                             layer_name = 'fc3',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=num_classes,\n",
    "                             name_scope = 'sd_graph',\n",
    "                             layer_name = 'fc4',\n",
    "                             use_relu=False)\n",
    "\n",
    "    drop_out = tf.nn.dropout(layer_fc4, 0.5)\n",
    "    y_pred = tf.nn.softmax(drop_out)\n",
    "    y_pred_cls = tf.argmax(y_pred, axis=1)        \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=drop_out, labels=y_true)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))        \n",
    "\n",
    "    #         y_pred = layer_fc4        \n",
    "    #         cost = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    #         optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "    #         # ## some more performance measures\n",
    "    #         correct_prediction = tf.equal(y_pred, y_true)\n",
    "    #         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_epochs, save_model=True,save_name= \"base_model\",restore_model=False,restore_name=None):\n",
    "    total_iterations = 0\n",
    "    done_train_imgs = 0\n",
    "    start_time = time.time()\n",
    "    start_batch=0\n",
    "    end_batch = train_batch_size\n",
    "    plot_accuracy=[]\n",
    "    plot_accuracy_epoch=[]\n",
    "    plot_training_size=[]\n",
    "    plot_training_size_epoch=[]\n",
    "    sum_accuracy = 0.0\n",
    "    n = 1\n",
    "    with sd_graph.as_default():\n",
    "        sd_saver = tf.train.Saver()\n",
    "    \n",
    "    with mask_graph.as_default():\n",
    "        mask_saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    with sd_graph.as_default():\n",
    "        session = tf.Session(graph = sd_graph)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "\n",
    "            #to save the model\n",
    "        for i in range(0, num_epochs):   \n",
    "            start_batch=0\n",
    "            end_batch = train_batch_size\n",
    "\n",
    "            print(\"Epoch:\", i + 1)\n",
    "\n",
    "            if restore_model==True:\n",
    "                if restore_name==None:\n",
    "                    print(\"No model file specified\")\n",
    "                    return\n",
    "                else:\n",
    "                    saver.restore(session,restore_name)\n",
    "\n",
    "            sum_accuracy = 0.0\n",
    "            n = 1\n",
    "            while end_batch < total_imgs:\n",
    "\n",
    "                train_data, mask_labels, left_mask, right_mask, labels = generate_batch(train_batch_size, img_shape, item_size, nitems)\n",
    "#                 train, mask_labels, labels = generate_batch(train_batch_size, img_shape, item_size, nitems)\n",
    "                if not len(train) and not len(labels) and not len(mask_labels):\n",
    "                    print(\"All images have been processed.\")\n",
    "                    break;\n",
    "                    \n",
    "#                 generate left and right masks for training    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "#                 x_batch, y_true_batch = next_batch(len(train), train, labels)\n",
    "                \n",
    "                feed_dict_train = {x: train_data,\n",
    "                           y_true: left_mask}\n",
    "\n",
    "                session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "                acc,co = session.run([accuracy, cost], feed_dict=feed_dict_train)\n",
    "                sum_accuracy += acc\n",
    "                n+=1\n",
    "                msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}, Loss: {2:>.4f}\"\n",
    "                print(msg.format(end_batch + 1, acc, co))\n",
    "                if end_batch % 100:\n",
    "                    plot_accuracy.append(acc)\n",
    "                    plot_training_size.append(end_batch + 1)\n",
    "\n",
    "                start_batch += train_batch_size\n",
    "                end_batch += train_batch_size\n",
    "\n",
    "            if save_model==True:\n",
    "                if save_name==None:\n",
    "                    print(\"No model specified, model not being saved\")\n",
    "                    return\n",
    "                else:\n",
    "                    save_path = saver.save(session, save_name)\n",
    "                    restore_model = True\n",
    "                    print(\"Model saved in file: %s\" % save_name)\n",
    "            plot_accuracy_epoch.append(sum_accuracy/n)\n",
    "            plot_training_size_epoch.append(i + 1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "        # Print the time-usage.\n",
    "        print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))  \n",
    "        print(plot_accuracy)\n",
    "        print(plot_training_size)\n",
    "        print(plot_accuracy_epoch)\n",
    "        print(plot_training_size_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = 10000\n",
    "train_batch_size = 50\n",
    "\n",
    "save_model = True\n",
    "save_name = modelsd_test\n",
    "restore_model=False\n",
    "restore_name=modelsd_test\n",
    "optimize(num_epochs=1, save_model=True,save_name=modelsd_test,restore_model=False,restore_name=modelsd_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_output_grey(iNp,depth_filter_to_see=0,cmap=\"gray\",figsize=(4,4)):\n",
    "    img_x = iNp[0,:,:]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.imshow(img_x, interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def see_output(iNp,depth_filter_to_see=0,cmap=\"gray\",figsize=(4,4)):\n",
    "    img_x = iNp[0,:,:,:]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if cmap == \"gray\":\n",
    "        plt.imshow(img_x, cmap=plt.get_cmap('gray'))\n",
    "    else:\n",
    "        plt.imshow(img_x, interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
