{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2, os, math, time\n",
    "from datetime import timedelta\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Configuration\n",
    "\"\"\"\n",
    "Data Configurations/Paths\n",
    "\"\"\"\n",
    "img_dir=\"../gen_script/data\"\n",
    "img_dir_same = img_dir + \"/1\"\n",
    "img_dir_diff = img_dir + \"/0\"\n",
    "\n",
    "##\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size3 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# Convolutional Layer 4.\n",
    "filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 256             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "1000000 (1000000,) 1000000\n",
      "499548 (499548,) 499548\n",
      "500452 (500452,) 500452\n"
     ]
    }
   ],
   "source": [
    "def change_label_dimensions(labels):\n",
    "    label_temp = np.zeros((len(labels), 2))\n",
    "    \n",
    "    for idx in range(0, len(labels)):\n",
    "        if labels[idx] == 1:\n",
    "            label_temp[idx][1] = 1\n",
    "        else:\n",
    "            label_temp[idx][0] = 1\n",
    "    \n",
    "    return label_temp\n",
    "\n",
    "def load_data(img_dir, label):\n",
    "        list_of_imgs = []\n",
    "        list_of_labels = []\n",
    "        for img in os.listdir(img_dir):\n",
    "                img = os.path.join(img_dir, img)\n",
    "#                 print(img)\n",
    "                if not img.endswith(\".png\"):\n",
    "                        continue\n",
    "\n",
    "                list_of_imgs.append(img)\n",
    "                list_of_labels.append(label)\n",
    "        data_labels = np.asarray(list_of_labels, dtype=np.int32)\n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "#         data_labels = np.array(list_of_labels)\n",
    "        return data_imgs, data_labels\n",
    "    \n",
    "def get_batch_images(data, label):\n",
    "        list_of_imgs = []\n",
    "        list_of_labels = []\n",
    "        for img, lbl in zip(data, label):\n",
    "#             print(img, lbl)\n",
    "            a = cv2.imread(img)\n",
    "            if a is None:\n",
    "                    print (\"Unable to read image{}\".format(img))\n",
    "                    continue\n",
    "            \n",
    "            flattened_a = a.flatten()\n",
    "#             flattened_a = tf.cast(flattened_a, tf.float32)\n",
    "                \n",
    "#             print(type(flattened_a.shape)) \n",
    "#             print(type(np.asarray(flattened_a, dtype=np.float32)))\n",
    "            \n",
    "            list_of_imgs.append(np.asarray(flattened_a, dtype=np.float32))\n",
    "            list_of_labels.append(lbl)\n",
    "\n",
    "        data_labels = np.asarray(list_of_labels, dtype=np.int32)\n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "        \n",
    "#         print(data_imgs)\n",
    "#         print(data_labels)\n",
    "        \n",
    "        return data_imgs, data_labels\n",
    "\n",
    "train_imgs_same, train_labels_same = load_data(img_dir_same, 0)\n",
    "train_imgs_diff, train_labels_diff = load_data(img_dir_diff, 1)\n",
    "\n",
    "# train_imgs = np.append(train_imgs_same, train_imgs_diff)\n",
    "train_imgs = np.concatenate([train_imgs_same, train_imgs_diff])\n",
    "train_lbls = np.append(train_labels_same, train_labels_diff)\n",
    "print(len(train_lbls))\n",
    "print(len(train_imgs), train_imgs.shape, len(train_lbls))\n",
    "print(len(train_imgs_same), train_imgs_same.shape, len(train_labels_same))\n",
    "print(len(train_imgs_diff), train_imgs_diff.shape, len(train_labels_diff))\n",
    "\n",
    "# print(train_imgs, train_lbls)\n",
    "train_data, train_labels = shuffle(train_imgs, train_lbls, random_state=0)\n",
    "\n",
    "train_labels = change_label_dimensions(train_labels)\n",
    "# print(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Batch Own Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that images are 60 pixels in each dimension.\n",
    "img_size = 60\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Number of colour channels for the images: 3 channel for RGB.\n",
    "num_channels = 3\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size, num_channels)\n",
    "\n",
    "\n",
    "# Number of classes, one class for same or different image\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Few Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (60,60,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b9671064e878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Plot the images and labels using our helper-function above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-fac14e62e8df>\u001b[0m in \u001b[0;36mplot_images\u001b[0;34m(images, cls_true, cls_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Plot image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Show true and predicted classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (60,60,3)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhIUvSY2w8CWpERa+JDXi/wEAlW8FcYU1/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd17e3cadd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first images from the train-set.\n",
    "images = train_data[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = train_labels[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Helper Functions for TF Graph Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_weights(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "#     return tf.Variable(initializer(shape))\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_bias(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(input,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   use_pooling=True):\n",
    "\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = new_weights(shape)\n",
    "    biases = new_bias(length=num_filters)\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                     filter=weights,\n",
    "                     strides=[1, 1, 1, 1],\n",
    "                     padding='SAME')\n",
    "    layer += biases\n",
    "\n",
    "    if use_pooling:\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 3, 3, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,\n",
    "                num_inputs,\n",
    "                num_outputs,\n",
    "                use_relu=True):\n",
    "\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_bias(length=num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(60), Dimension(60), Dimension(3)]),\n",
       " <tf.Tensor 'y_true_1:0' shape=(?, 2) dtype=float32>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat*num_channels], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, axis=1)\n",
    "x_image.shape, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer1_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                            num_input_channels=num_channels,\n",
    "                                            filter_size=filter_size1,\n",
    "                                            num_filters=num_filters1,\n",
    "                                            use_pooling=True)\n",
    "\n",
    "layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "                                           num_input_channels=num_filters1,\n",
    "                                           filter_size=filter_size2,\n",
    "                                           num_filters=num_filters2,\n",
    "                                           use_pooling=True)\n",
    "\n",
    "layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "                                           num_input_channels=num_filters2,\n",
    "                                           filter_size=filter_size3,\n",
    "                                           num_filters=num_filters3,\n",
    "                                           use_pooling=True)\n",
    "\n",
    "layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "                                           num_input_channels=num_filters3,\n",
    "                                           filter_size=filter_size4,\n",
    "                                           num_filters=num_filters4,\n",
    "                                           use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer4_conv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=True)\n",
    "\n",
    "\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=False)\n",
    "\n",
    "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=fc_size,\n",
    "                         use_relu=False)\n",
    "\n",
    "drop_out = tf.nn.dropout(layer_fc3, 0.5)\n",
    "\n",
    "layer_fc4 = new_fc_layer(input=drop_out,\n",
    "                         num_inputs=fc_size,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False)\n",
    "\n",
    "##Normalize the numbers(apply softmax!)\n",
    "\n",
    "y_pred = tf.nn.softmax(layer_fc4)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc4,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "## some more performance measures\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Tensorflow on Defined Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "train_batch_size = 64\n",
    "\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    total_iterations = 0\n",
    "    done_train_imgs = 0\n",
    "    start_time = time.time()\n",
    "    start_batch=0\n",
    "    end_batch = train_batch_size\n",
    "    \n",
    "#     for i in range(total_iterations, total_iterations + num_iterations):\n",
    "    while True:\n",
    "        train = train_data[start_batch:end_batch]\n",
    "        labels = train_labels[start_batch:end_batch]\n",
    "        train, labels = get_batch_images(train, labels)\n",
    "        if not len(train) and not len(labels):\n",
    "            print(\"All images have been processed.\")\n",
    "            break;\n",
    "            \n",
    "        x_batch, y_true_batch = next_batch(train_batch_size, train, labels)\n",
    "#         x_batch, y_true_batch = train_data[done_train_images:done_train_imgs+train_batch_size]\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                   y_true: y_true_batch}\n",
    "        \n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        \n",
    "        if total_iterations%1000==0:    \n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "            print(msg.format(total_iterations + 1, acc))\n",
    "            \n",
    "            # Update the total number of iterations performed.\n",
    "#         done_train_imgs+=train_batch_size\n",
    "        start_batch += train_batch_size\n",
    "        end_batch += train_batch_size\n",
    "        total_iterations +=1\n",
    "        \n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:  53.1%\n",
      "Optimization Iteration:   1001, Training Accuracy:  53.1%\n",
      "Optimization Iteration:   2001, Training Accuracy:  50.0%\n",
      "Optimization Iteration:   3001, Training Accuracy:  50.0%\n",
      "Optimization Iteration:   4001, Training Accuracy:  46.9%\n",
      "Optimization Iteration:   5001, Training Accuracy:  50.0%\n",
      "Optimization Iteration:   6001, Training Accuracy:  59.4%\n",
      "Optimization Iteration:   7001, Training Accuracy:  43.8%\n",
      "Optimization Iteration:   8001, Training Accuracy:  60.9%\n",
      "Optimization Iteration:   9001, Training Accuracy:  56.2%\n",
      "Optimization Iteration:  10001, Training Accuracy:  40.6%\n",
      "Optimization Iteration:  11001, Training Accuracy:  53.1%\n",
      "Optimization Iteration:  12001, Training Accuracy:  48.4%\n",
      "Optimization Iteration:  13001, Training Accuracy:  70.3%\n",
      "Optimization Iteration:  14001, Training Accuracy:  67.2%\n",
      "Optimization Iteration:  15001, Training Accuracy:  46.9%\n",
      "All images have been processed.\n",
      "Time usage: 0:04:49\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_example_errors(cls_pred, correct):\n",
    "    # This function is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # correct is a boolean array whether the predicted class\n",
    "    # is equal to the true class for each image in the test-set.\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = data.test.images[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = data.test.cls[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])\n",
    "    \n",
    "def plot_confusion_matrix(cls_pred):\n",
    "    # This is called from print_test_accuracy() below.\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.test.cls\n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the test-set into smaller batches of this size.\n",
    "test_batch_size = 256\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y_true: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))\n",
    "\n",
    "    # Plot some examples of mis-classifications, if desired.\n",
    "    if show_example_errors:\n",
    "        print(\"Example errors:\")\n",
    "        plot_example_errors(cls_pred=cls_pred, correct=correct)\n",
    "\n",
    "    # Plot the confusion matrix, if desired.\n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plot_confusion_matrix(cls_pred=cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_true_batch = data.test.next_batch(64)\n",
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
