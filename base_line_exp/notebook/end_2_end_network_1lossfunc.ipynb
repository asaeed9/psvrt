{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../original_images/psvrt.py:3: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 132, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 427, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/base_events.py\", line 1440, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-9d25e3f2c43d>\", line 1, in <module>\n",
      "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2131, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-107>\", line 2, in matplotlib\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/magics/pylab.py\", line 99, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3049, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/pylabtools.py\", line 311, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 231, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/__init__.py\", line 1410, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"/Users/as186233/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2, os, math, time, sys\n",
    "from datetime import timedelta\n",
    "from sklearn.utils import shuffle\n",
    "sys.path.append('../../original_images')\n",
    "from gen_data_batch import generate_batch, generate_batch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_output_grey(iNp,depth_filter_to_see=0,cmap=\"gray\",figsize=(4,4)):\n",
    "    img_x = iNp[0,:,:]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.imshow(img_x, interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def see_output(iNp,depth_filter_to_see=0,cmap=\"gray\",figsize=(4,4)):\n",
    "    img_x = iNp[0,:,:,:]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    if cmap == \"gray\":\n",
    "        plt.imshow(img_x, cmap=plt.get_cmap('gray'))\n",
    "    else:\n",
    "        plt.imshow(img_x, interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "    plt.show()\n",
    "    \n",
    "def normalise(tensor):\n",
    "    return tf.div(\n",
    "   tf.subtract(\n",
    "      tensor, \n",
    "      tf.reduce_min(tensor)\n",
    "   ), \n",
    "   tf.subtract(\n",
    "      tf.reduce_max(tensor), \n",
    "      tf.reduce_min(tensor)\n",
    "   )\n",
    ") \n",
    "\n",
    "def new_weights(shape, layer_name):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape), name=layer_name+'_W')\n",
    "\n",
    "def new_bias(length, layer_name):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]), name=layer_name+'_b')\n",
    "\n",
    "def new_conv_layer(input,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   name_scope,\n",
    "                   layer_name='',\n",
    "                   use_pooling=True):\n",
    "\n",
    "    with tf.name_scope(name_scope):\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "        weights = new_weights(shape, layer_name)\n",
    "        biases = new_bias(num_filters, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.nn.conv2d(input=input, filter=weights, strides=[1,1,1,1], padding='SAME'), biases, name=layer_name)\n",
    "#         print('layer:', layer)\n",
    "        if use_pooling:\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, 2, 2, 1],\n",
    "                                   strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME', name=layer_name+'_max')\n",
    "        layer = tf.nn.relu(layer, name=layer_name+'_activation')\n",
    "        \n",
    "#         print('maxpooled layer:', layer)\n",
    "        \n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,\n",
    "                num_inputs,\n",
    "                num_outputs,\n",
    "                name_scope,\n",
    "                layer_name='',\n",
    "                use_relu=True):\n",
    "    \n",
    "    with tf.name_scope(name_scope):\n",
    "        weights = new_weights([num_inputs, num_outputs], layer_name)\n",
    "        biases = new_bias(num_outputs, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.matmul(input, weights),biases,name=layer_name)\n",
    "    #     layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer, layer_name+'_activation')\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def argmax_2d(tensor, bat_len):\n",
    "    \n",
    "    inp_tensor_shape = tf.shape(tensor)\n",
    "    nimgs = inp_tensor_shape[0]\n",
    "    img_dims = inp_tensor_shape[1:]\n",
    "    img_len = inp_tensor_shape[-1]\n",
    "    flat_img = tf.reshape(tensor, [-1, tf.reduce_prod(img_dims)])\n",
    "\n",
    "    # # argmax of the flat tensor\n",
    "    argmax_x = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) // tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "    argmax_y = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) % tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "\n",
    "    res = tf.stack([argmax_x, argmax_y, tf.zeros([bat_len], dtype=tf.float64)], axis = 1)\n",
    "    \n",
    "    return res\n",
    "\n",
    "# def extract_patch(orig_vec, mask_vec):\n",
    "#     batch_len = tf.shape(mask_vec)[0]\n",
    "#     input_shape = (img_shape[0], img_shape[1])\n",
    "#     rows, cols = input_shape[0], input_shape[1]\n",
    "#     item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "#     d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "#     subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "#     ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "#     d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "#     subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "#     subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "#     subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "\n",
    "#     gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), mask_vec)\n",
    "\n",
    "#     subm_dims = tf.shape(gather_exp)\n",
    "#     gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "#     reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "#     pred_crds = argmax_2d(reduced_mat, batch_len)\n",
    "    \n",
    "#     pred_crds = tf.cast(pred_crds, dtype = tf.int64)\n",
    "    \n",
    "#     itms = tf.map_fn(lambda idx: tf.cast(tf.slice(orig_vec[tf.cast(idx, tf.int64), :, :, :],pred_crds[tf.cast(idx, tf.int64), :], [item_size[0],item_size[1], 3]), dtype = tf.float64), tf.cast(tf.range(batch_len), dtype = tf.float64))\n",
    "# #     itms = tf.map_fn(lambda idx: tf.cast(tf.slice(tf.squeeze(vec[idx:idx+1, :, :]), tf.squeeze(pred_crds[idx:idx+1, :]), [item_size[0], item_size[1]]), dtype=tf.int32), tf.range(batch_len))\n",
    "    \n",
    "#     return itms\n",
    "\n",
    "\n",
    "def extract_patch(orig_vec, mask_vec):\n",
    "    batch_len = tf.shape(mask_vec)[0]\n",
    "    input_shape = (img_shape[0], img_shape[1])\n",
    "    rows, cols = input_shape[0], input_shape[1]\n",
    "    item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "    d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "    subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "    ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "    d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "    subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "    subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "    subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "\n",
    "    gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), mask_vec)\n",
    "\n",
    "    subm_dims = tf.shape(gather_exp)\n",
    "    gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "    reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "    pred_crds = argmax_2d(reduced_mat, batch_len)\n",
    "    \n",
    "    pred_crds = tf.cast(pred_crds, dtype = tf.int64, name= \"predicted_crds\")\n",
    "    \n",
    "    itms = tf.map_fn(lambda idx: tf.cast(tf.slice(orig_vec[tf.cast(idx, tf.int64), :, :, :],pred_crds[tf.cast(idx, tf.int64), :], [item_size[0],item_size[1], 3]), dtype = tf.float64), tf.cast(tf.range(batch_len), dtype = tf.float64))\n",
    "#     itms = tf.map_fn(lambda idx: tf.cast(tf.slice(tf.squeeze(vec[idx:idx+1, :, :]), tf.squeeze(pred_crds[idx:idx+1, :]), [item_size[0], item_size[1]]), dtype=tf.int32), tf.range(batch_len))\n",
    "    \n",
    "    return itms\n",
    "\n",
    "def restore_see_layer(orig, input_name, model_name=None, var_name=None):\n",
    "    result = []\n",
    "    with tf.Session('', tf.Graph()) as s:\n",
    "        with s.graph.as_default():\n",
    "            if ((model_name != None) and var_name != None):\n",
    "                saver = tf.train.import_meta_graph(model_name+\".meta\")\n",
    "                saver.restore(s, model_name)\n",
    "                fd = {input_name +':0': orig}\n",
    "#                 print(fd.shape)\n",
    "                for var in var_name:\n",
    "                    var=var+\":0\"\n",
    "#                     result = 0\n",
    "\n",
    "                    result.append(s.run(var, feed_dict=fd))\n",
    "    return result[0], result[1] \n",
    "\n",
    "def test_mask_sd(batch_size, model_mask_sd): \n",
    "    np.set_printoptions(suppress=True)\n",
    "    sd_accuracy = 0.\n",
    "    mask_accuracy = 0.\n",
    "    sig_mask_accuracy = 0.\n",
    "    \n",
    "    test_set, mask_test_labels, test_labels = generate_batch_1(batch_size, img_shape, item_size, nitems) \n",
    "    \n",
    "#     mask_logits,  sd = restore_see_layer(orig=test_set, input_name = 'x', model_name=model_mask_sd, var_name=['train_mask/mask_6/fc3', 'mask_sd/mask_conv_layer/sd_y_pred'])\n",
    "    mask_logits, sd = restore_see_layer(orig=test_set, input_name = 'x', model_name=model_mask_sd, var_name=['train_mask/sigmoid_output', 'mask_sd/mask_conv_layer/sd_y_pred'])\n",
    "\n",
    "#     mask_logits[mask_logits < 0 ] = 0\n",
    "    \n",
    "#     print('Predicted Mask Logits: ', mask_logits[0].shape)\n",
    "#     print('Predicted Mask: ', pred_mask.shape)\n",
    "#     print('Original Mask: ', mask_test_labels)\n",
    "#     print('Test SD: ', test_set)\n",
    "\n",
    "    sd_accuracy = np.sum(np.argmax(sd, axis=1) == np.argmax(test_labels, axis=1))/batch_size\n",
    "\n",
    "#     cor_prd_imgs = np.sum([True for pm, tm  in zip(pred_mask, mask_test_labels) if np.allclose(pm, tm)])    \n",
    "#     sig_cor_prd_imgs = np.sum([True for pm, tm  in zip(pred_mask, mask_test_labels) if np.allclose(pm, tm)])    \n",
    "\n",
    "#     print(cor_prd_imgs)\n",
    "#     mask_accuracy = cor_prd_imgs/batch_size\n",
    "#     sig_mask_accuracy = sig_cor_prd_imgs/batch_size\n",
    "    \n",
    "#     print('Mask Test Accuracy:{0:>.4f}, Sigmoid Test Accuracy:{1:>.4f}'.format(mask_accuracy, sig_mask_accuracy))\n",
    "\n",
    "#     print('Mask Test Accuracy:{0:>.4f}, SD Test Accuracy:{1:>.4f}'.format(mask_accuracy, sd_accuracy))\n",
    "    \n",
    "\n",
    "    \n",
    "    return test_set, mask_accuracy, sd_accuracy, mask_logits, mask_test_labels\n",
    "\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "def plotNNFilter(units):\n",
    "    print(units.shape)\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\")\n",
    "        \n",
    "\n",
    "def getActivations(layer, feed_dict_train):\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        units = session.run(layer, feed_dict=feed_dict_train)\n",
    "    \n",
    "        plotNNFilter(units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bat = 1\n",
    "\n",
    "# train_data, mask_labels, labels = generate_batch_1(test_bat, img_shape, item_size, nitems) \n",
    "\n",
    "feed_dict_train = {x: train_data,\n",
    "           mask_y_true: mask_labels}\n",
    "\n",
    "see_output(np.reshape(train_data, [test_bat, img_shape[0], img_shape[1], img_shape[2]]))\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     units = session.run(num_features, feed_dict=feed_dict_train)\n",
    "\n",
    "# print(m_num_features)\n",
    "# getActivations(mask_layer0_conv0, feed_dict_train)\n",
    "# print(\"******\")\n",
    "# getActivations(mask_layer1_conv1, feed_dict_train)\n",
    "# print(\"******\")\n",
    "# getActivations(mask_layer2_conv2, feed_dict_train)\n",
    "# print(\"******\")\n",
    "# getActivations(mask_layer3_conv3, feed_dict_train)\n",
    "# getActivations(mask_layer4_conv4, feed_dict_train)\n",
    "getActivations(mask_layer5_conv5, feed_dict_train)\n",
    "# getActivations(mask_layer_fc3, feed_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bat = 1\n",
    "# test_set, mask_test_labels, test_labels = generate_batch_1(test_bat, img_shape, item_size, nitems) \n",
    "# orig_img = np.reshape(test_set, [test_bat, img_shape[0], img_shape[1], img_shape[2]])\n",
    "\n",
    "# resh_mask_test_lbls = np.reshape(mask_test_labels, [test_bat, img_shape[0], img_shape[1]])\n",
    "\n",
    "# see_output(orig_img, figsize=(4,4))\n",
    "# see_output_grey(resh_mask_test_lbls, figsize = (4,4))\n",
    "\n",
    "orig_img, mask_acc, pred_crds, mask_logits, mask_test_lbls = test_mask_sd(test_bat, single_patch_model)\n",
    "resh_mask_test_lbls = np.reshape(mask_test_lbls, [test_bat, img_shape[0], img_shape[1]])\n",
    "resh_orig_img = np.reshape(orig_img, [test_bat, img_shape[0], img_shape[1], img_shape[2]])\n",
    "\n",
    "resh_logits = np.reshape(mask_logits, [test_bat, img_shape[0], img_shape[1]])\n",
    "# resh_mask = np.reshape(mask, [test_bat, img_shape[0], img_shape[1]])\n",
    "\n",
    "print(resh_logits)\n",
    "# print(list(resh_mask))\n",
    "# print(pred_crds)\n",
    "# print(resh_mask[0].shape)\n",
    "# print(resh_mask_test_lbls)\n",
    "\n",
    "# print(np.array_equal(np.squeeze(resh_mask[0])), np.squeeze(np.flatten(resh_mask_test_lbls[0]))))\n",
    "\n",
    "# resh_logits[resh_logits < 0] = 0\n",
    "see_output(resh_orig_img)\n",
    "see_output_grey(resh_logits)\n",
    "\n",
    "# see_output_grey(resh_mask, figsize=(8,8))\n",
    "see_output_grey(resh_mask_test_lbls)\n",
    "\n",
    "\n",
    "# resh_mask_test_lbls\n",
    "# img_x = iNp[0,:,:]\n",
    "# fig = plt.figure(figsize=(8,8))\n",
    "# plt.imshow(np.squeeze(resh_mask_test_lbls), interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_patch_model = \"SD/single_patch_model.ckpt\"\n",
    "total_imgs = 128\n",
    "train_batch_size = 64\n",
    "img_shape = (10,10,3)\n",
    "item_size = (2,2)\n",
    "nitems = 2\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_shape[0]*img_shape[1]*img_shape[2]], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_shape[0], img_shape[1], img_shape[2]])\n",
    "\n",
    "mask_y_true = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1]], name='mask_y_true')\n",
    "mask_y_true_cls = tf.argmax(mask_y_true, axis=1) \n",
    "\n",
    "sd_y_true = tf.placeholder(tf.float32, shape=[None, 2], name='sd_y_true')\n",
    "sd_y_true_cls = tf.argmax(sd_y_true, axis=1)        \n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "name_scope = 'train_mask'\n",
    "\n",
    "# mask_graph layer configurations\n",
    "# mask_graph layer configurations\n",
    "m_filter_size0 = 3          # Convolution filters(kernel) are 4 x 4 pixels.\n",
    "m_num_filters0 = 16         # There are 16 of these filters.\n",
    "\n",
    "m_filter_size1 = 3          # Convolution filters are 4 x 4 pixels.\n",
    "m_num_filters1 = 32         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "m_filter_size2 = 3          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters2 = 64         # There are 16 of these filters.\n",
    "\n",
    "m_filter_size3 = 3          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters3 = 128         # There are 4 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "m_filter_size4 = 3          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters4 = 256         # There are 32 of these filters.\n",
    "\n",
    "m_filter_size5 = 3          # Convolution filters are 2 x 2 pixels.\n",
    "m_num_filters5 = 512         # There are 16 of these filters.\n",
    "\n",
    "\n",
    "# Fully-connected layer.\n",
    "m_fc_size = 2000             # Number of neurons in fully-connected layer.\n",
    "\n",
    "\n",
    "\"\"\"SD Network Layer Architecture\"\"\"\n",
    "filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size3 = 4          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# Convolutional Layer 4.\n",
    "filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 256             # Number of neurons in fully-connected layer.\n",
    "\n",
    "with tf.name_scope(name_scope):\n",
    "# First Convolution Layer\n",
    "    layer_name = 'mask_conv_layer'\n",
    "    shape = [m_filter_size0, m_filter_size0, img_shape[2], m_num_filters0]\n",
    "    mask_weights = tf.Variable(initializer(shape), name=layer_name+'_W')  \n",
    "    mask_biases = tf.Variable(tf.constant(0.05, shape=[m_num_filters0]), name=layer_name+'_b')\n",
    "\n",
    "    mask_layer0_conv0, weights_conv0 = new_conv_layer(input=x_image,\n",
    "                                                num_input_channels=img_shape[2],\n",
    "                                                filter_size=m_filter_size0,\n",
    "                                                num_filters=m_num_filters0,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv1',\n",
    "                                                use_pooling=False)\n",
    "#     layer0_conv0_drpout = tf.nn.dropout(layer0_conv0, 0.3, name=\"drop_out\")\n",
    "\n",
    "    mask_layer1_conv1, weights_conv1 = new_conv_layer(input=mask_layer0_conv0,\n",
    "                                                num_input_channels=m_num_filters0,\n",
    "                                                filter_size=m_filter_size1,\n",
    "                                                num_filters=m_num_filters1,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv2',\n",
    "                                                use_pooling=False)\n",
    "\n",
    "#     layer1_conv1_drpout = tf.nn.dropout(layer1_conv1, 0.3, name=\"drop_out\")\n",
    "\n",
    "    mask_layer2_conv2, weights_conv2 =  new_conv_layer(input=mask_layer1_conv1,\n",
    "                                               num_input_channels=m_num_filters1,\n",
    "                                               filter_size=m_filter_size2,\n",
    "                                               num_filters=m_num_filters2,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv3',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    mask_layer3_conv3, weights_conv3 =  new_conv_layer(input=mask_layer2_conv2,\n",
    "                                               num_input_channels=m_num_filters2,\n",
    "                                               filter_size=m_filter_size3,\n",
    "                                               num_filters=m_num_filters3,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv4',\n",
    "                                               use_pooling=False)\n",
    "\n",
    "    mask_layer4_conv4, weights_conv4 =  new_conv_layer(input=mask_layer3_conv3,\n",
    "                                               num_input_channels=m_num_filters3,\n",
    "                                               filter_size=m_filter_size4,\n",
    "                                               num_filters=m_num_filters4,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv5',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "\n",
    "    mask_layer5_conv5, weights_conv5 =  new_conv_layer(input=mask_layer4_conv4,\n",
    "                                               num_input_channels=m_num_filters4,\n",
    "                                               filter_size=m_filter_size5,\n",
    "                                               num_filters=m_num_filters5,\n",
    "                                                 name_scope = 'mask',\n",
    "                                                 layer_name = 'conv6',\n",
    "                                               use_pooling=False)\n",
    "    \n",
    "#     layer6_conv6, weights_conv6 =  new_conv_layer(input=layer5_conv5,\n",
    "#                                            num_input_channels=m_num_filters5,\n",
    "#                                            filter_size=m_filter_size6,\n",
    "#                                            num_filters=m_num_filters6,\n",
    "#                                              name_scope = 'mask',\n",
    "#                                              layer_name = 'conv7',\n",
    "#                                            use_pooling=True)\n",
    "\n",
    "    m_layer_flat, m_num_features = flatten_layer(mask_layer5_conv5)\n",
    "#     print(layer_flat)\n",
    "#     layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "#                              num_inputs=num_features,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc1',\n",
    "#                              use_relu=False)\n",
    "#     print('layer_fc1:', layer_fc1)\n",
    "#     layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc2',\n",
    "#                              use_relu=False)\n",
    "\n",
    "#     layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc3',\n",
    "#                              use_relu=False)\n",
    "#     mask_drop_out = tf.nn.dropout(layer_fc3, 0.5, name=\"drop_out\")\n",
    "\n",
    "    mask_layer_fc3 = new_fc_layer(input=m_layer_flat,\n",
    "                             num_inputs=m_num_features,\n",
    "                             num_outputs=img_shape[0] * img_shape[1],\n",
    "                             name_scope = 'mask',\n",
    "                             layer_name = 'fc3',\n",
    "                             use_relu=False)\n",
    "\n",
    "#     mask_drop_out = tf.nn.dropout(mask_layer_fc4, 0.5, name=\"drop_out\")\n",
    "#     y_pred = tf.nn.softmax(mask_drop_out, name=\"softmax_output\")\n",
    "    y_pred = mask_layer_fc3\n",
    "    \n",
    "    # round numbers less than 0.5 to zero;\n",
    "    # by making them negative and taking the maximum with 0\n",
    "#     differentiable_round = tf.maximum(y_pred-0.499,0)\n",
    "    # scale the remaining numbers (0 to 0.5) to greater than 1\n",
    "    # the other half (zeros) is not affected by multiplication\n",
    "#     differentiable_round = differentiable_round * 10000\n",
    "    # take the minimum with 1\n",
    "#     y_pred_round = tf.minimum(differentiable_round, 1)\n",
    "#     print(differentiable_round)\n",
    "\n",
    "    y_pred_sigmoid = tf.nn.sigmoid(y_pred, name=\"sigmoid_output\")\n",
    "    y_pred_sigmoid = tf.reshape(y_pred_sigmoid, [-1, img_shape[0], img_shape[1]])\n",
    "    \n",
    "    batch_len = tf.shape(y_pred_sigmoid)[0]\n",
    "    input_shape = (img_shape[0], img_shape[1])\n",
    "    rows, cols = input_shape[0], input_shape[1]\n",
    "    item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "    d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "    subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "    ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "    d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "    subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "    subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "    subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "    \n",
    "    gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), y_pred_sigmoid)\n",
    "\n",
    "    subm_dims = tf.shape(gather_exp)\n",
    "    gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "    reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "    \n",
    "    inp_tensor_shape = tf.shape(reduced_mat)\n",
    "    nimgs = inp_tensor_shape[0]\n",
    "    img_dims = inp_tensor_shape[1:]\n",
    "    img_len = inp_tensor_shape[-1]\n",
    "    flat_img = tf.reshape(reduced_mat, [-1, tf.reduce_prod(img_dims)])\n",
    "\n",
    "    _, top_idx = tf.nn.top_k(flat_img, nitems)\n",
    "    topx_idx = tf.squeeze(tf.cast(tf.reshape(top_idx, [-1, 1]), tf.float32) // tf.cast(img_len, tf.float32))\n",
    "    topy_idx = tf.squeeze(tf.cast(tf.reshape(top_idx, [-1, 1]), tf.float32) % tf.cast(img_len, tf.float32))\n",
    "    \n",
    "    res12 = tf.stack([topx_idx, topy_idx, tf.zeros([batch_len * nitems], dtype=tf.float32)], axis = 1, name= \"pred_crds12\")    \n",
    "    \n",
    "    pred_crds12 = tf.cast(res12, dtype = tf.int32, name= \"predicted_crds12\") \n",
    "    pred_crds = tf.map_fn(lambda idx: pred_crds12[idx: idx + 2], tf.range(0, batch_len * nitems, nitems), dtype = tf.int32)\n",
    "   \n",
    "  \n",
    "    items = tf.map_fn(lambda img_idx: tf.reshape(tf.map_fn(lambda itm_idx: tf.slice(x_image[img_idx, :, :, :],\n",
    "                                                      pred_crds[img_idx, itm_idx, :], \n",
    "                                                      [item_size[0],item_size[1], 3])\n",
    "                                               , tf.range(nitems), dtype = tf.float32), \n",
    "                                                [item_size[0] * 2, item_size[0], 3])\n",
    "                         , tf.range(batch_len), dtype = tf.float32)\n",
    "\n",
    "#     mask_loss = tf.square(mask_y_true - y_pred)\n",
    "#     mask_cost = tf.reduce_mean(mask_loss)\n",
    "    \n",
    "#     train_op1 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(mask_loss)\n",
    "    \n",
    "    ### some more performance measures\n",
    "#     mask_correct_prediction = tf.equal(y_pred, mask_y_true)\n",
    "#     mask_accuracy = tf.reduce_mean(tf.cast(mask_correct_prediction, tf.float32))        \n",
    "\n",
    "name_scope = 'mask_sd'\n",
    "with tf.name_scope(name_scope):\n",
    "\n",
    "    input_sd = tf.cast(items, dtype = tf.float32, name=layer_name + \"/input_sd\")\n",
    "    \n",
    "    layer1_conv1, weights_conv1 = new_conv_layer(input=input_sd,\n",
    "                                                num_input_channels=img_shape[2],\n",
    "                                                filter_size=filter_size1,\n",
    "                                                num_filters=num_filters1,\n",
    "                                                 name_scope = 'mask_sd_graph',\n",
    "                                                 layer_name = 'conv1',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "    layer2_conv2, weights_conv2 = new_conv_layer(input=layer1_conv1,\n",
    "                                                num_input_channels=num_filters1,\n",
    "                                                filter_size=filter_size2,\n",
    "                                                num_filters=num_filters2,\n",
    "                                                 name_scope = 'mask_sd_graph',\n",
    "                                                 layer_name = 'conv2',\n",
    "                                                use_pooling=True)\n",
    "\n",
    "    layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "                                               num_input_channels=num_filters2,\n",
    "                                               filter_size=filter_size3,\n",
    "                                               num_filters=num_filters3,\n",
    "                                                 name_scope = 'mask_sd_graph',\n",
    "                                                 layer_name = 'conv3',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "                                               num_input_channels=num_filters3,\n",
    "                                               filter_size=filter_size4,\n",
    "                                               num_filters=num_filters4,\n",
    "                                                 name_scope = 'mask_sd_graph',\n",
    "                                                 layer_name = 'conv4',\n",
    "                                               use_pooling=True)\n",
    "\n",
    "    layer_flat, num_features = flatten_layer(layer4_conv4)       \n",
    "\n",
    "    layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                     num_inputs=num_features,\n",
    "                     num_outputs=fc_size,\n",
    "                     name_scope = 'mask_sd_graph',\n",
    "                     layer_name = 'fc1',\n",
    "                     use_relu=True)\n",
    "\n",
    "    layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=fc_size,\n",
    "                             name_scope = 'mask_sd_graph',\n",
    "                             layer_name = 'fc2',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=fc_size,\n",
    "                             name_scope = 'mask_sd_graph',\n",
    "                             layer_name = 'fc3',\n",
    "                             use_relu=False)\n",
    "\n",
    "    layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "                             num_inputs=fc_size,\n",
    "                             num_outputs=num_classes,\n",
    "                             name_scope = 'mask_sd_graph',\n",
    "                             layer_name = 'fc4',\n",
    "                             use_relu=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    sd_y_pred = tf.nn.softmax(layer_fc4, name=layer_name + \"/sd_y_pred\")\n",
    "    sd_y_pred_cls = tf.argmax(sd_y_pred, axis=1)\n",
    "    \n",
    "    sd_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=sd_y_pred, labels=sd_y_true)\n",
    "    \n",
    "#     train_op1 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(mask_loss)\n",
    "    train_op2 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(sd_loss)\n",
    "#     final_train_op = tf.group(train_op1, train_op2)    \n",
    "\n",
    "    sd_cost = tf.reduce_mean(sd_loss)\n",
    "    \n",
    "    sd_correct_prediction = tf.equal(sd_y_pred_cls, sd_y_true_cls)\n",
    "    sd_accuracy = tf.reduce_mean(tf.cast(sd_correct_prediction, tf.float32))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_epochs, save_model=True,save_name= \"base_model\",restore_model=False,restore_name=None):\n",
    "    total_iterations = 0\n",
    "    done_train_imgs = 0\n",
    "    start_time = time.time()\n",
    "    start_batch=0\n",
    "    end_batch = train_batch_size\n",
    "    test_batch_size = 256\n",
    "    plot_accuracy=[]\n",
    "    plot_accuracy_epoch=[]\n",
    "    plot_training_size=[]\n",
    "    plot_training_size_epoch=[]\n",
    "    plot_mask = []\n",
    "    plot_mask_sd = []\n",
    "    plot_sd = []\n",
    "    plot_sr = []\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sum_accuracy = 0.0\n",
    "    n = 1\n",
    "\n",
    "    session = tf.Session()         \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "            #to save the model\n",
    "    for i in range(0, num_epochs):   \n",
    "        start_batch=0\n",
    "        end_batch = train_batch_size\n",
    "\n",
    "        print(\"Epoch:\", i + 1)\n",
    "\n",
    "        if restore_model==True:\n",
    "            if restore_name==None:\n",
    "                print(\"No model file specified\")\n",
    "                return\n",
    "            else:\n",
    "                saver.restore(session,restore_name)\n",
    "\n",
    "        sum_accuracy = 0.0\n",
    "        n = 1\n",
    "        while end_batch < total_imgs:\n",
    "\n",
    "            train_data, mask_labels, labels = generate_batch_1(train_batch_size, img_shape, item_size, nitems) \n",
    "#             print(train_data.shape)\n",
    "#             print(train_data[0])\n",
    "            if not len(train_data) and not len(labels) and not len(mask_labels):\n",
    "                print(\"All images have been processed.\")\n",
    "                break;\n",
    "\n",
    "            feed_dict_train = {x: train_data,\n",
    "                              sd_y_true: labels}\n",
    "\n",
    "#             feed_dict_train = {x: train_data,\n",
    "#                        mask_y_true: mask_labels}\n",
    "\n",
    "            session.run(train_op2, feed_dict=feed_dict_train)\n",
    "\n",
    "            sd_acc, sd_co = session.run([sd_correct_prediction, sd_cost], feed_dict=feed_dict_train)            \n",
    "            msg = \"Optimization Iteration: {0:>6},  SD Loss: {1:>.4f}\"\n",
    "\n",
    "#             msg = \"Optimization Iteration: {0:>6}, SD Training Accuracy: {1:>6.1%}, SD Loss: {2:>.4f}\"\n",
    "            print(msg.format(end_batch + 1,  sd_co))\n",
    "\n",
    "#             sd_acc, sd_co, itms = session.run([sd_correct_prediction, sd_cost, items], feed_dict=feed_dict_train)            \n",
    "#             msg = \"Optimization Iteration: {0:>6}, SD Training Accuracy: {1:>6.1%}, Mask Loss: {2:>.4f}\"\n",
    "#             print(msg.format(end_batch + 1, sd_acc, sd_co))\n",
    "\n",
    "#             print(list(train_data[0]))\n",
    "#             print('Predicted Cords: ', list(pred_crd))\n",
    "\n",
    "#             fig=plt.figure(figsize=(8, 8))\n",
    "#             columns = 8\n",
    "#             rows = 8\n",
    "#             for i in range(1, columns*rows):\n",
    "#                 img = itms[i]\n",
    "#                 fig.add_subplot(rows, columns, i)\n",
    "#                 plt.imshow(img)\n",
    "#             plt.show()            \n",
    "            \n",
    "            start_batch += train_batch_size\n",
    "            end_batch += train_batch_size\n",
    "        if save_model==True:\n",
    "            if save_name==None:\n",
    "                print(\"No model specified, model not being saved\")\n",
    "                return\n",
    "            else:\n",
    "                save_path = saver.save(session, save_name)\n",
    "                restore_model = True\n",
    "                print(\"Model saved in file: %s\" % save_name)\n",
    "    \n",
    "        s_time = time.time()                    \n",
    "        _, mask_acc, sd_acc, _, _ = test_mask_sd(test_batch_size, save_name)\n",
    "        print('Test Accuracy:', sd_acc)\n",
    "#         plot_mask.append(mask_acc)\n",
    "        plot_sd.append(sd_acc)\n",
    "        e_time = time.time()\n",
    "        time_dif = e_time - s_time\n",
    "        print(\"Test Mask SD Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))  \n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif))))) \n",
    "    print('plot mask accuracy:')\n",
    "#     print(plot_mask)\n",
    "    print(plot_sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../original_images/psvrt.py:253: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  running_orientation += np.arctan(y / x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:     65,  SD Loss: 0.6892\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 2\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6973\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 3\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6939\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 4\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6923\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 5\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6942\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 6\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6934\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 7\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6968\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 8\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6936\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 9\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6912\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Epoch: 10\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Optimization Iteration:     65,  SD Loss: 0.6902\n",
      "Model saved in file: SD/single_patch_model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from SD/single_patch_model.ckpt\n",
      "Test Mask SD Time usage: 0:00:01\n",
      "Time usage: 0:00:19\n",
      "plot mask accuracy:\n",
      "[0.44140625, 0.48046875, 0.53515625, 0.51171875, 0.48828125, 0.4609375, 0.5, 0.51953125, 0.51953125, 0.4140625]\n"
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "save_name = single_patch_model\n",
    "restore_model=False\n",
    "restore_name=single_patch_model\n",
    "optimize(num_epochs=10, save_model=True,save_name=single_patch_model,restore_model=restore_model,restore_name=single_patch_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data.shape)\n",
    "session = tf.Session()         \n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "feed_dict_train = {x: train_data,\n",
    "           mask_y_true: mask_labels}\n",
    "\n",
    "session.run(final_train_op, feed_dict=feed_dict_train)\n",
    "mask_acc, mask_co, sd_acc, sd_co = session.run(, )            \n",
    "msg = \"Optimization Iteration: {0:>6}, Mask Training Accuracy: {1:>6.1%}, Mask Loss: {2:>.4f}, SD Training Accuracy: {1:>6.1%}, SD Loss: {2:>.4f}\"\n",
    "print(msg.format(1, mask_acc, mask_co, sd_acc, sd_co))\n",
    "\n",
    "# print(session.run(tf.squeeze([y_pred_norm]), feed_dict=feed_dict_train))\n",
    "# print(session.run(tf.squeeze([items]), feed_dict=feed_dict_train))\n",
    "\n",
    "# print(np.array(s_ii))\n",
    "# print(np.array(s_jj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(graph=tf.get_default_graph()):\n",
    "    return [t for op in graph.get_operations() for t in op.values()]\n",
    "get_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gand Balaaa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "item_size = (3,3)\n",
    "\"\"\"\n",
    "This section of code extracts a Single patch from an input image using convolution logic. Additionally, it should be dynamic!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# z_vec = tf.zeros([5,5])\n",
    "# z_vec[:, ]\n",
    "\n",
    "vec1 = tf.constant(np.array([[0,0,0,0,0],\n",
    "                             [0,0,0,0,0],\n",
    "                             [0,1,1,1,0],\n",
    "                             [0,1,1,1,0],\n",
    "                             [0,1,1,1,0]]), dtype=tf.float32)\n",
    "# vec1 = tf.constant(np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]), dtype=tf.float32)\n",
    "\n",
    "# vec1 = tf.constant(np.array([[0,0,0,0],[1,1,0,0],[1,1,0,0],[0,0,0,0]]))\n",
    "# vec = tf.stack([vec1, vec2])\n",
    "input_shape = tf.shape(vec1)\n",
    "rows, cols = input_shape[0], input_shape[1]\n",
    "item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "\n",
    "gather_exp = tf.gather_nd(vec1, subm_st)\n",
    "subm_dims = tf.shape(gather_exp)\n",
    "gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[0] * subm_dims[1], subm_dims[2], subm_dims[3]])\n",
    "reduced_mat = tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(gather_exp))[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# res = argmax_2d(reduced_mat)\n",
    "# print(reduced_mat)\n",
    "# pred_crds = tf.argmax(reduced_mat, axis = -1)\n",
    "# slice_exp = tf.slice(vec1, pred_crds[:2], [item_size[0],item_size[1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(np.array(sess.run([vec1])))\n",
    "#     print(sess.run([ii, jj]))\n",
    "#     print(sess.run([d_ii, d_jj]))    \n",
    "#     print(sess.run([subm_ii, subm_jj]))\n",
    "#     print('subm_st: ', sess.run(subm_st))\n",
    "    print(sess.run([reduced_mat]))\n",
    "#     print(sess.run([res]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section of code extracts a Single patch from an input image using convolution logic.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# z_vec = tf.zeros([5,5])\n",
    "# z_vec[:, ]\n",
    "\n",
    "# vec1 = tf.constant(np.array([[1,1,0,0,0],[1,1,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]), dtype=tf.float32)\n",
    "# vec1 = tf.constant(np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]), dtype=tf.float32)\n",
    "\n",
    "# vec1 = tf.constant(np.array([[0,0,0,0],[1,1,0,0],[1,1,0,0],[0,0,0,0]]))\n",
    "# vec = tf.stack([vec1, vec2])\n",
    "# input_shape = tf.shape(vec1)\n",
    "# rows, cols = input_shape[0], input_shape[1]\n",
    "# item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "# d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "# subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "# ii, jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "# d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "# subm_ii = ii[:subm_rows - 1, :subm_cols - 1, tf.newaxis, tf.newaxis] + d_ii\n",
    "# subm_jj = jj[:subm_rows - 1, :subm_cols - 1, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "# subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "\n",
    "# gather_exp = tf.gather_nd(vec1, subm_st)\n",
    "# subm_dims = tf.shape(gather_exp)\n",
    "# gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[0] * subm_dims[1], subm_dims[2], subm_dims[3]])\n",
    "# reduced_mat = tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(gather_exp))[-1]\n",
    "# res = argmax_2d(reduced_mat)\n",
    "\n",
    "vec = tf.constant(np.array([[0.06943432, 0.07020187, 0.06474971, 0.06497546, 0.07147662, 0.07259881,\n",
    "  0.05938388, 0.05955945, 0.06461262],\n",
    "                            [0.0666275,  0.07140117, 0.06605807, 0.0650783,  0.07378063, 0.07307022,\n",
    "  0.05833262, 0.059543,   0.06464229],\n",
    " [0.06950475, 0.07092812, 0.06614418, 0.06343528, 0.07138074, 0.07305786,\n",
    "  0.05641314, 0.05814052, 0.06446858],\n",
    " [0.06943432, 0.07020187, 0.06474971, 0.06497546, 0.07147662, 0.07259881,\n",
    "  0.05938388, 0.05955945, 0.06461262]]))\n",
    "\n",
    "_, top_idx = tf.nn.top_k(vec, 1)\n",
    "tf.argmax(vec)\n",
    "\n",
    "# slice_exp = tf.slice(vec1, pred_crds[:2], [item_size[0],item_size[1]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(np.array(sess.run([top_idx])))\n",
    "    print(sess.run(tf.argmax(vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 2\n",
    "img_shape = (4,4,3)\n",
    "item_size = (2,2)\n",
    "nitems = 1\n",
    "num_classes = 2\n",
    "train_data, mask_labels, labels = generate_batch_1(train_batch_size, img_shape, item_size, nitems)\n",
    "\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(mask_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_size = (2,2)\n",
    "\n",
    "\"\"\"\n",
    "This section of code extracts a Single patch from a batch of input images using convolution logic.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# vec1 = tf.constant([[0.5000534,  0.50027275, 0.5007929,  0.5007636 ],\n",
    "#   [0.49887955, 0.49952933, 0.7321372,  0.73167676],\n",
    "#   [0.49823606, 0.49942428, 0.73225975, 0.7310878],\n",
    "#   [0.49861944, 0.49947158, 0.50059104, 0.50023127]], dtype = tf.float32)\n",
    "\n",
    "vec1 = tf.constant([[0.4978641 , 0.4956584 , 0.4949304 , 0.49439728, 0.49452817,\n",
    "        0.4958884 , 0.4963342 , 0.49593773, 0.49512833, 0.49550587,\n",
    "        0.49697465, 0.4975773 , 0.49848527, 0.4995232 , 0.49985358,\n",
    "        0.49962163],\n",
    "       [0.49549603, 0.48905388, 0.48708674, 0.4877914 , 0.48884934,\n",
    "        0.48947975, 0.49131534, 0.49057013, 0.4885408 , 0.48942864,\n",
    "        0.49258766, 0.49458462, 0.4973427 , 0.49927768, 0.5003911 ,\n",
    "        0.49981767],\n",
    "       [0.49481988, 0.4877937 , 0.48733968, 0.4927347 , 0.49941167,\n",
    "        0.50185966, 0.50076365, 0.49616677, 0.49190122, 0.4903961 ,\n",
    "        0.4916535 , 0.4947803 , 0.4989497 , 0.5014477 , 0.50265616,\n",
    "        0.50091815],\n",
    "       [0.49613094, 0.49168405, 0.49528345, 0.5066927 , 0.51926625,\n",
    "        0.5290109 , 0.5255986 , 0.5159217 , 0.50637305, 0.49645182,\n",
    "        0.49435067, 0.49748772, 0.5015879 , 0.50414276, 0.50530475,\n",
    "        0.50302917],\n",
    "       [0.49826103, 0.49734956, 0.5063491 , 0.5230674 , 0.5447407 ,\n",
    "        0.5585421 , 0.5575325 , 0.5429373 , 0.52183753, 0.5054217 ,\n",
    "        0.49832663, 0.49789354, 0.5008529 , 0.5051209 , 0.50628245,\n",
    "        0.50336254],\n",
    "       [0.49848902, 0.49875587, 0.5104595 , 0.5306728 , 0.5607434 ,\n",
    "        0.57694983, 0.5740651 , 0.55711186, 0.5304589 , 0.5115793 ,\n",
    "        0.49730125, 0.4946889 , 0.49860254, 0.50537866, 0.50765926,\n",
    "        0.5034961 ],\n",
    "       [0.49692908, 0.49479628, 0.5038091 , 0.52817476, 0.5567202 ,\n",
    "        0.5766375 , 0.5714736 , 0.552144  , 0.5294191 , 0.5081695 ,\n",
    "        0.49444747, 0.49119878, 0.49804845, 0.5044677 , 0.50759953,\n",
    "        0.5038293 ],\n",
    "       [0.49604037, 0.4896978 , 0.49505466, 0.51802427, 0.5386894 ,\n",
    "        0.5555474 , 0.5553916 , 0.53705835, 0.5181776 , 0.4994972 ,\n",
    "        0.48908502, 0.48629636, 0.49497905, 0.50344086, 0.5064416 ,\n",
    "        0.50285274],\n",
    "       [0.4959775 , 0.4886112 , 0.49028432, 0.50256294, 0.51662964,\n",
    "        0.528004  , 0.5277405 , 0.5141205 , 0.5007169 , 0.4924641 ,\n",
    "        0.4862668 , 0.4877291 , 0.49525216, 0.50327724, 0.5075504 ,\n",
    "        0.50384295],\n",
    "       [0.49701646, 0.49050507, 0.488937  , 0.49323085, 0.49995005,\n",
    "        0.5067111 , 0.5054026 , 0.49548566, 0.48946118, 0.4891706 ,\n",
    "        0.4925281 , 0.49593738, 0.50055325, 0.5059984 , 0.508257  ,\n",
    "        0.50412536],\n",
    "       [0.49876624, 0.4948571 , 0.49127162, 0.49093327, 0.4913554 ,\n",
    "        0.49337223, 0.49405855, 0.4897231 , 0.48721927, 0.4937782 ,\n",
    "        0.5014173 , 0.50512713, 0.5050719 , 0.5046365 , 0.50445545,\n",
    "        0.5015145 ],\n",
    "       [0.5013082 , 0.5007559 , 0.4990454 , 0.49523154, 0.4920312 ,\n",
    "        0.49025568, 0.49094442, 0.4903319 , 0.4915165 , 0.5000091 ,\n",
    "        0.5073812 , 0.50864124, 0.5036821 , 0.4995103 , 0.49828368,\n",
    "        0.49889606],\n",
    "       [0.5034798 , 0.50726914, 0.50701916, 0.50465554, 0.49810913,\n",
    "        0.49325868, 0.49389797, 0.4949675 , 0.49890733, 0.50624317,\n",
    "        0.5095126 , 0.5065519 , 0.4998414 , 0.49380058, 0.49281672,\n",
    "        0.49675387],\n",
    "       [0.5039327 , 0.5092165 , 0.51090914, 0.51043284, 0.50547206,\n",
    "        0.4994874 , 0.4976094 , 0.5004631 , 0.50473547, 0.5099943 ,\n",
    "        0.5088441 , 0.50153214, 0.49306548, 0.48713982, 0.48760238,\n",
    "        0.49469545],\n",
    "       [0.50362045, 0.5088228 , 0.51180565, 0.5127194 , 0.5098044 ,\n",
    "        0.5039876 , 0.5009916 , 0.50176984, 0.50630695, 0.5080712 ,\n",
    "        0.5039945 , 0.49686325, 0.49104849, 0.4872204 , 0.48728675,\n",
    "        0.49425945],\n",
    "       [0.5022053 , 0.50436574, 0.5055876 , 0.5068979 , 0.5054464 ,\n",
    "        0.5024895 , 0.5010205 , 0.5010002 , 0.50180304, 0.50210136,\n",
    "        0.50021565, 0.49727255, 0.4957635 , 0.49458537, 0.4945194 ,\n",
    "        0.49758774]], dtype = tf.float32)\n",
    "\n",
    "# vec1 = tf.constant([[1,1,1,0,0],\n",
    "#                     [1,1,1,0,0],\n",
    "#                     [1,1,1,0,0],\n",
    "#                     [0,0,0,0,0], \n",
    "#                     [0,0,0,0,0]], dtype=tf.float32)\n",
    "# # vec1 = tf.constant(np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]]), dtype=tf.float32)\n",
    "\n",
    "# vec2 = tf.constant([[0,0,0,0,0],\n",
    "#                     [0,0,0,0,0], \n",
    "#                     [1,1,1,0,0],\n",
    "#                     [1,1,1,0,0],\n",
    "#                     [1,1,1,0,0]], dtype=tf.float32)\n",
    "\n",
    "# vec3 = tf.constant([[0,0,0,0,0],\n",
    "#                     [0,0,0,0,0], \n",
    "#                     [0,0,0,0,0],\n",
    "#                     [0,0,0,0,0],\n",
    "#                     [0,0,0,0,0]], dtype=tf.float32)\n",
    "\n",
    "# vec1 = tf.constant([[0, 0, 0, 0],\n",
    "#   [0, 0, 0, 0],\n",
    "#   [0, 0, 0, 0],\n",
    "#   [0, 0, 0, 1]])\n",
    "\n",
    "# vec2 = tf.constant([[0, 0, 0, 0],\n",
    "#   [0, 0, 0, 0],\n",
    "#   [0, 0, 0, 0],\n",
    "#   [0, 0, 0, 0]])\n",
    "\n",
    "# orig_data = tf.convert_to_tensor(np.reshape(train_data, [2, 4,4,3]))\n",
    "# print(mask_labels)\n",
    "vec = tf.reshape(vec1, [1, 16,16])\n",
    "\n",
    "batch_len = tf.shape(vec)[0]\n",
    "input_shape = (img_shape[0], img_shape[1])\n",
    "rows, cols = input_shape[0], input_shape[1]\n",
    "item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "\n",
    "gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), vec)\n",
    "\n",
    "subm_dims = tf.shape(gather_exp)\n",
    "gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "#     pred_crds = argmax_2d(reduced_mat, batch_len)\n",
    "\n",
    "inp_tensor_shape = tf.shape(reduced_mat)\n",
    "nimgs = inp_tensor_shape[0]\n",
    "img_dims = inp_tensor_shape[1:]\n",
    "img_len = inp_tensor_shape[-1]\n",
    "flat_img = tf.reshape(reduced_mat, [-1, tf.reduce_prod(img_dims)])\n",
    "\n",
    "# # argmax of the flat tensor\n",
    "argmax_x = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) // tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "argmax_y = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) % tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "\n",
    "res = tf.stack([argmax_x, argmax_y, tf.zeros([batch_len], dtype=tf.float64)], axis = 1, name= \"pred_crds\")    \n",
    "\n",
    "pred_crds = tf.cast(res, dtype = tf.int64, name= \"predicted_crds\")\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(vec))\n",
    "#     print(sess.run(vec[0:1, :, :]))\n",
    "#     print(sess.run(pred_crds[0:, :]))\n",
    "    print(sess.run(pred_crds))\n",
    "#     print(sess.run(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Working Complete Network\"\"\"\n",
    "# # Placeholders\n",
    "# x = tf.placeholder(tf.float32, shape=[None, img_shape[0]*img_shape[1]*img_shape[2]], name='x')\n",
    "# x_image = tf.reshape(x, [-1, img_shape[0], img_shape[1], img_shape[2]])\n",
    "\n",
    "# mask_y_true = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1]], name='mask_y_true')\n",
    "# mask_y_true_cls = tf.argmax(mask_y_true, axis=1) \n",
    "\n",
    "# sd_y_true = tf.placeholder(tf.float32, shape=[None, 2], name='sd_y_true')\n",
    "# sd_y_true_cls = tf.argmax(sd_y_true, axis=1)        \n",
    "\n",
    "# initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# name_scope = 'train_mask'\n",
    "\n",
    "# # mask_graph layer configurations\n",
    "# m_filter_size0 = 16          # Convolution filters(kernel) are 4 x 4 pixels.\n",
    "# m_num_filters0 = 16         # There are 16 of these filters.\n",
    "\n",
    "# m_filter_size1 = 8          # Convolution filters are 4 x 4 pixels.\n",
    "# m_num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# # Convolutional Layer 2.\n",
    "# m_filter_size2 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters2 = 16         # There are 16 of these filters.\n",
    "\n",
    "# m_filter_size3 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters3 = 4         # There are 4 of these filters.\n",
    "\n",
    "# # Convolutional Layer 3.\n",
    "# m_filter_size4 = 4          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters4 = 32         # There are 32 of these filters.\n",
    "\n",
    "# m_filter_size5 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters5 = 16         # There are 16 of these filters.\n",
    "\n",
    "\n",
    "# # Fully-connected layer.\n",
    "# m_fc_size = 2000             # Number of neurons in fully-connected layer.\n",
    "\n",
    "\n",
    "# \"\"\"SD Network Layer Architecture\"\"\"\n",
    "# filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "# num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# # Convolutional Layer 2.\n",
    "# filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# # Convolutional Layer 3.\n",
    "# filter_size3 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# # Convolutional Layer 4.\n",
    "# filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# # Fully-connected layer.\n",
    "# fc_size = 256             # Number of neurons in fully-connected layer.\n",
    "\n",
    "# with tf.name_scope(name_scope):\n",
    "# # First Convolution Layer\n",
    "#     layer_name = 'mask_conv_layer'\n",
    "#     shape = [m_filter_size0, m_filter_size0, img_shape[2], m_num_filters0]\n",
    "#     mask_weights = tf.Variable(initializer(shape), name=layer_name+'_W')  \n",
    "#     mask_biases = tf.Variable(tf.constant(0.05, shape=[m_num_filters0]), name=layer_name+'_b')\n",
    "\n",
    "#     layer0_conv0, weights_conv0 = new_conv_layer(input=x_image,\n",
    "#                                                 num_input_channels=img_shape[2],\n",
    "#                                                 filter_size=m_filter_size0,\n",
    "#                                                 num_filters=m_num_filters0,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv1',\n",
    "#                                                 use_pooling=True)\n",
    "\n",
    "#     layer1_conv1, weights_conv1 = new_conv_layer(input=layer0_conv0,\n",
    "#                                                 num_input_channels=m_num_filters0,\n",
    "#                                                 filter_size=m_filter_size1,\n",
    "#                                                 num_filters=m_num_filters1,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv2',\n",
    "#                                                 use_pooling=True)\n",
    "\n",
    "#     layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "#                                                num_input_channels=m_num_filters1,\n",
    "#                                                filter_size=m_filter_size2,\n",
    "#                                                num_filters=m_num_filters2,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv3',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "#                                                num_input_channels=m_num_filters2,\n",
    "#                                                filter_size=m_filter_size3,\n",
    "#                                                num_filters=m_num_filters3,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv4',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "#                                                num_input_channels=m_num_filters3,\n",
    "#                                                filter_size=m_filter_size4,\n",
    "#                                                num_filters=m_num_filters4,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv5',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "\n",
    "#     layer5_conv5, weights_conv5 =  new_conv_layer(input=layer4_conv4,\n",
    "#                                                num_input_channels=m_num_filters4,\n",
    "#                                                filter_size=m_filter_size5,\n",
    "#                                                num_filters=m_num_filters5,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv6',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer_flat, num_features = flatten_layer(layer5_conv5)\n",
    "# #     print(layer_flat)\n",
    "#     layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "#                              num_inputs=num_features,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc1',\n",
    "#                              use_relu=True)\n",
    "# #     print('layer_fc1:', layer_fc1)\n",
    "#     layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc2',\n",
    "#                              use_relu=False)\n",
    "\n",
    "#     layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc3',\n",
    "#                              use_relu=False)\n",
    "\n",
    "#     mask_layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=img_shape[0] * img_shape[1],\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc4',\n",
    "#                              use_relu=False)\n",
    "\n",
    "# #     mask_drop_out = tf.nn.dropout(mask_layer_fc4, 0.5, name=\"drop_out\")\n",
    "# #     y_pred = tf.nn.softmax(mask_drop_out, name=\"softmax_output\")\n",
    "#     y_pred = mask_layer_fc4\n",
    "    \n",
    "#     y_pred_norm = tf.to_int32(normalise(y_pred), name= layer_name + \"/y_pred_norm\")\n",
    "#     y_pred_norm = tf.reshape(y_pred_norm, [-1, img_shape[0],img_shape[1]])\n",
    "\n",
    "# #     orig_vec = tf.convert_to_tensor(np.reshape(train_data, [train_batch_size, img_shape[0],img_shape[1],img_shape[2]]))\n",
    "# #     mask_vec = tf.convert_to_tensor(np.reshape(mask_labels, [train_batch_size, img_shape[0],img_shape[1]]))    \n",
    "    \n",
    "# #     Item Extraction\n",
    "#     items = extract_patch(x_image, y_pred_norm)\n",
    "    \n",
    "#     mask_loss = tf.square(mask_y_true - y_pred)\n",
    "#     mask_cost = tf.reduce_mean(mask_loss)\n",
    "\n",
    "#     ### some more performance measures\n",
    "#     mask_correct_prediction = tf.equal(y_pred, mask_y_true)\n",
    "#     mask_accuracy = tf.reduce_mean(tf.cast(mask_correct_prediction, tf.float32))        \n",
    "\n",
    "# name_scope = 'mask_sd'\n",
    "# with tf.name_scope(name_scope):\n",
    "\n",
    "#     input_sd = tf.cast(items, dtype = tf.float32, name=layer_name + \"/input_sd\")\n",
    "    \n",
    "#     layer1_conv1, weights_conv1 = new_conv_layer(input=input_sd,\n",
    "#                                                 num_input_channels=img_shape[2],\n",
    "#                                                 filter_size=filter_size1,\n",
    "#                                                 num_filters=num_filters1,\n",
    "#                                                  name_scope = 'mask_sd_graph',\n",
    "#                                                  layer_name = 'conv1',\n",
    "#                                                 use_pooling=True)\n",
    "\n",
    "#     layer2_conv2, weights_conv2 = new_conv_layer(input=layer1_conv1,\n",
    "#                                                 num_input_channels=num_filters1,\n",
    "#                                                 filter_size=filter_size2,\n",
    "#                                                 num_filters=num_filters2,\n",
    "#                                                  name_scope = 'mask_sd_graph',\n",
    "#                                                  layer_name = 'conv2',\n",
    "#                                                 use_pooling=True)\n",
    "\n",
    "#     layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "#                                                num_input_channels=num_filters2,\n",
    "#                                                filter_size=filter_size3,\n",
    "#                                                num_filters=num_filters3,\n",
    "#                                                  name_scope = 'mask_sd_graph',\n",
    "#                                                  layer_name = 'conv3',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "#                                                num_input_channels=num_filters3,\n",
    "#                                                filter_size=filter_size4,\n",
    "#                                                num_filters=num_filters4,\n",
    "#                                                  name_scope = 'mask_sd_graph',\n",
    "#                                                  layer_name = 'conv4',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer_flat, num_features = flatten_layer(layer4_conv4)       \n",
    "\n",
    "#     layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "#                      num_inputs=num_features,\n",
    "#                      num_outputs=fc_size,\n",
    "#                      name_scope = 'mask_sd_graph',\n",
    "#                      layer_name = 'fc1',\n",
    "#                      use_relu=True)\n",
    "\n",
    "#     layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "#                              num_inputs=fc_size,\n",
    "#                              num_outputs=fc_size,\n",
    "#                              name_scope = 'mask_sd_graph',\n",
    "#                              layer_name = 'fc2',\n",
    "#                              use_relu=False)\n",
    "\n",
    "#     layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "#                              num_inputs=fc_size,\n",
    "#                              num_outputs=fc_size,\n",
    "#                              name_scope = 'mask_sd_graph',\n",
    "#                              layer_name = 'fc3',\n",
    "#                              use_relu=False)\n",
    "\n",
    "#     layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "#                              num_inputs=fc_size,\n",
    "#                              num_outputs=num_classes,\n",
    "#                              name_scope = 'mask_sd_graph',\n",
    "#                              layer_name = 'fc4',\n",
    "#                              use_relu=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     sd_y_pred = tf.nn.softmax(layer_fc4, name=layer_name + \"/sd_y_pred\")\n",
    "#     sd_y_pred_cls = tf.argmax(sd_y_pred, axis=1)\n",
    "    \n",
    "#     sd_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=sd_y_pred, labels=sd_y_true)\n",
    "    \n",
    "#     train_op1 = tf.train.AdamOptimizer().minimize(mask_loss)\n",
    "#     train_op2 = tf.train.AdamOptimizer().minimize(sd_loss)\n",
    "#     final_train_op = tf.group(train_op1, train_op2)    \n",
    "\n",
    "#     sd_cost = tf.reduce_mean(sd_loss)\n",
    "    \n",
    "#     sd_correct_prediction = tf.equal(sd_y_pred_cls, sd_y_true_cls)\n",
    "#     sd_accuracy = tf.reduce_mean(tf.cast(sd_correct_prediction, tf.float32))        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Placeholders\n",
    "# x = tf.placeholder(tf.float32, shape=[None, img_shape[0]*img_shape[1]*img_shape[2]], name='x')\n",
    "# x_image = tf.reshape(x, [-1, img_shape[0], img_shape[1], img_shape[2]])\n",
    "\n",
    "# mask_y_true = tf.placeholder(tf.float32, shape=[None, img_shape[0] * img_shape[1]], name='mask_y_true')\n",
    "# mask_y_true_cls = tf.argmax(mask_y_true, axis=1) \n",
    "\n",
    "# sd_y_true = tf.placeholder(tf.float32, shape=[None, 2], name='sd_y_true')\n",
    "# sd_y_true_cls = tf.argmax(sd_y_true, axis=1)        \n",
    "\n",
    "# initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# name_scope = 'train_mask'\n",
    "\n",
    "# # mask_graph layer configurations\n",
    "# # mask_graph layer configurations\n",
    "# m_filter_size0 = 16          # Convolution filters(kernel) are 4 x 4 pixels.\n",
    "# m_num_filters0 = 16         # There are 16 of these filters.\n",
    "\n",
    "# m_filter_size1 = 8          # Convolution filters are 4 x 4 pixels.\n",
    "# m_num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# # Convolutional Layer 2.\n",
    "# m_filter_size2 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters2 = 16         # There are 16 of these filters.\n",
    "\n",
    "# m_filter_size3 = 8          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters3 = 4         # There are 4 of these filters.\n",
    "\n",
    "# # Convolutional Layer 3.\n",
    "# m_filter_size4 = 4          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters4 = 32         # There are 32 of these filters.\n",
    "\n",
    "# m_filter_size5 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# m_num_filters5 = 16         # There are 16 of these filters.\n",
    "\n",
    "\n",
    "# # Fully-connected layer.\n",
    "# m_fc_size = 2000             # Number of neurons in fully-connected layer.\n",
    "\n",
    "\n",
    "# \"\"\"SD Network Layer Architecture\"\"\"\n",
    "# filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "# num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# # Convolutional Layer 2.\n",
    "# filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# # Convolutional Layer 3.\n",
    "# filter_size3 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# # Convolutional Layer 4.\n",
    "# filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "# num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# # Fully-connected layer.\n",
    "# fc_size = 256             # Number of neurons in fully-connected layer.\n",
    "\n",
    "# with tf.name_scope(name_scope):\n",
    "# # First Convolution Layer\n",
    "#     layer_name = 'mask_conv_layer'\n",
    "#     shape = [m_filter_size0, m_filter_size0, img_shape[2], m_num_filters0]\n",
    "#     mask_weights = tf.Variable(initializer(shape), name=layer_name+'_W')  \n",
    "#     mask_biases = tf.Variable(tf.constant(0.05, shape=[m_num_filters0]), name=layer_name+'_b')\n",
    "\n",
    "#     layer0_conv0, weights_conv0 = new_conv_layer(input=x_image,\n",
    "#                                                 num_input_channels=img_shape[2],\n",
    "#                                                 filter_size=m_filter_size0,\n",
    "#                                                 num_filters=m_num_filters0,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv1',\n",
    "#                                                 use_pooling=True)\n",
    "# #     layer0_conv0_drpout = tf.nn.dropout(layer0_conv0, 0.3, name=\"drop_out\")\n",
    "\n",
    "#     layer1_conv1, weights_conv1 = new_conv_layer(input=layer0_conv0,\n",
    "#                                                 num_input_channels=m_num_filters0,\n",
    "#                                                 filter_size=m_filter_size1,\n",
    "#                                                 num_filters=m_num_filters1,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv2',\n",
    "#                                                 use_pooling=True)\n",
    "\n",
    "# #     layer1_conv1_drpout = tf.nn.dropout(layer1_conv1, 0.3, name=\"drop_out\")\n",
    "\n",
    "#     layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "#                                                num_input_channels=m_num_filters1,\n",
    "#                                                filter_size=m_filter_size2,\n",
    "#                                                num_filters=m_num_filters2,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv3',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "#                                                num_input_channels=m_num_filters2,\n",
    "#                                                filter_size=m_filter_size3,\n",
    "#                                                num_filters=m_num_filters3,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv4',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "#     layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "#                                                num_input_channels=m_num_filters3,\n",
    "#                                                filter_size=m_filter_size4,\n",
    "#                                                num_filters=m_num_filters4,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv5',\n",
    "#                                                use_pooling=True)\n",
    "\n",
    "\n",
    "#     layer5_conv5, weights_conv5 =  new_conv_layer(input=layer4_conv4,\n",
    "#                                                num_input_channels=m_num_filters4,\n",
    "#                                                filter_size=m_filter_size5,\n",
    "#                                                num_filters=m_num_filters5,\n",
    "#                                                  name_scope = 'mask',\n",
    "#                                                  layer_name = 'conv6',\n",
    "#                                                use_pooling=True)\n",
    "    \n",
    "# #     layer6_conv6, weights_conv6 =  new_conv_layer(input=layer5_conv5,\n",
    "# #                                            num_input_channels=m_num_filters5,\n",
    "# #                                            filter_size=m_filter_size6,\n",
    "# #                                            num_filters=m_num_filters6,\n",
    "# #                                              name_scope = 'mask',\n",
    "# #                                              layer_name = 'conv7',\n",
    "# #                                            use_pooling=True)\n",
    "\n",
    "#     layer_flat, num_features = flatten_layer(layer5_conv5)\n",
    "# #     print(layer_flat)\n",
    "#     layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "#                              num_inputs=num_features,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc1',\n",
    "#                              use_relu=False)\n",
    "# #     print('layer_fc1:', layer_fc1)\n",
    "#     layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=m_fc_size,\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc2',\n",
    "#                              use_relu=False)\n",
    "\n",
    "# #     layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "# #                              num_inputs=m_fc_size,\n",
    "# #                              num_outputs=m_fc_size,\n",
    "# #                              name_scope = 'mask',\n",
    "# #                              layer_name = 'fc3',\n",
    "# #                              use_relu=False)\n",
    "# #     mask_drop_out = tf.nn.dropout(layer_fc3, 0.5, name=\"drop_out\")\n",
    "\n",
    "#     mask_layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "#                              num_inputs=m_fc_size,\n",
    "#                              num_outputs=img_shape[0] * img_shape[1],\n",
    "#                              name_scope = 'mask',\n",
    "#                              layer_name = 'fc3',\n",
    "#                              use_relu=False)\n",
    "\n",
    "# #     mask_drop_out = tf.nn.dropout(mask_layer_fc4, 0.5, name=\"drop_out\")\n",
    "# #     y_pred = tf.nn.softmax(mask_drop_out, name=\"softmax_output\")\n",
    "#     y_pred = mask_layer_fc3\n",
    "    \n",
    "#     # round numbers less than 0.5 to zero;\n",
    "#     # by making them negative and taking the maximum with 0\n",
    "# #     differentiable_round = tf.maximum(y_pred-0.499,0)\n",
    "#     # scale the remaining numbers (0 to 0.5) to greater than 1\n",
    "#     # the other half (zeros) is not affected by multiplication\n",
    "# #     differentiable_round = differentiable_round * 10000\n",
    "#     # take the minimum with 1\n",
    "# #     y_pred_round = tf.minimum(differentiable_round, 1)\n",
    "# #     print(differentiable_round)\n",
    "\n",
    "#     y_pred_sigmoid = tf.nn.sigmoid(y_pred, name=\"sigmoid_output\")\n",
    "#     y_pred_sigmoid = tf.reshape(y_pred_sigmoid, [-1, img_shape[0], img_shape[1]])\n",
    "    \n",
    "#     batch_len = tf.shape(y_pred_sigmoid)[0]\n",
    "#     input_shape = (img_shape[0], img_shape[1])\n",
    "#     rows, cols = input_shape[0], input_shape[1]\n",
    "#     item_shape = tf.Variable([item_size[0], item_size[1]])\n",
    "#     d_rows, d_cols = item_shape[0], item_shape[1]\n",
    "#     subm_rows, subm_cols = rows - d_rows + 1, cols - d_cols + 1\n",
    "\n",
    "#     ii, jj = tf.meshgrid(tf.range(d_rows), tf.range(d_cols), indexing='ij')\n",
    "#     d_ii, d_jj = tf.meshgrid(tf.range(subm_rows), tf.range(subm_cols), indexing='ij')\n",
    "\n",
    "#     subm_ii = ii[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_ii\n",
    "#     subm_jj = jj[:subm_rows, :subm_cols, tf.newaxis, tf.newaxis] + d_jj\n",
    "\n",
    "#     subm_st = tf.stack([subm_ii, subm_jj], axis=-1)\n",
    "    \n",
    "#     gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), y_pred_sigmoid)\n",
    "\n",
    "#     subm_dims = tf.shape(gather_exp)\n",
    "#     gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "#     reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "    \n",
    "#     inp_tensor_shape = tf.shape(reduced_mat)\n",
    "#     nimgs = inp_tensor_shape[0]\n",
    "#     img_dims = inp_tensor_shape[1:]\n",
    "#     img_len = inp_tensor_shape[-1]\n",
    "#     flat_img = tf.reshape(reduced_mat, [-1, tf.reduce_prod(img_dims)])\n",
    "\n",
    "#     # # argmax of the flat tensor\n",
    "#     argmax_x = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) // tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "#     argmax_y = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) % tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "\n",
    "#     res = tf.stack([argmax_x, argmax_y, tf.zeros([batch_len], dtype=tf.float64)], axis = 1, name= \"pred_crds1\")    \n",
    "    \n",
    "#     pred_crds = tf.cast(res, dtype = tf.int64, name= \"predicted_crds\")    \n",
    "    \n",
    "#     itms = tf.map_fn(lambda idx: tf.cast(tf.slice(x_image[tf.cast(idx, tf.int64), :, :, :],pred_crds[tf.cast(idx, tf.int64), :], [item_size[0],item_size[1], 3]), dtype = tf.float64), tf.cast(tf.range(batch_len), dtype = tf.float64), name= \"original_items\")\n",
    "\n",
    "\n",
    "#     mask_loss = tf.square(mask_y_true - y_pred)\n",
    "#     mask_cost = tf.reduce_mean(mask_loss)\n",
    "    \n",
    "#     train_op1 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(mask_loss)\n",
    "\n",
    "# #     y_pred = tf.reshape(y_pred, [-1, img_shape[0] * img_shape[1]])\n",
    "# #     y_pred_round = tf.reshape(y_pred_round, [-1, img_shape[0], img_shape[1]])\n",
    "    \n",
    "# #     items = extract_patch(x_image, y_pred_sigmoid)\n",
    "\n",
    "\n",
    "# #     mask_loss = tf.reduce_sum(tf.map_fn(lambda idx: tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred_ind[:,tf.cast(idx, dtype = tf.int32),:], labels=mask_y_true[:, tf.cast(idx, dtype = tf.int32), :]), tf.cast(tf.range(img_shape[0] * img_shape[1]), dtype = tf.float32)))\n",
    "# #     print(mask_loss)\n",
    "# #     mask_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred_round, labels=mask_y_true)\n",
    "\n",
    "\n",
    "# #     gather_exp = tf.map_fn(lambda mask: tf.gather_nd(mask, subm_st), y_pred_sigmoid)\n",
    "\n",
    "# #     subm_dims = tf.shape(gather_exp)\n",
    "# #     gather_exp = tf.reshape(gather_exp, [-1,  subm_dims[1] * subm_dims[2], subm_dims[3], subm_dims[4]])\n",
    "# #     reduced_mat = tf.map_fn(lambda mask: tf.scan(lambda a, b: tf.multiply(a, b), tf.squeeze(mask))[-1], gather_exp)\n",
    "# # #     pred_crds = argmax_2d(reduced_mat, batch_len)\n",
    "    \n",
    "# #     inp_tensor_shape = tf.shape(reduced_mat)\n",
    "# #     nimgs = inp_tensor_shape[0]\n",
    "# #     img_dims = inp_tensor_shape[1:]\n",
    "# #     img_len = inp_tensor_shape[-1]\n",
    "# #     flat_img = tf.reshape(reduced_mat, [-1, tf.reduce_prod(img_dims)])\n",
    "\n",
    "# #     # # argmax of the flat tensor\n",
    "# #     argmax_x = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) // tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "# #     argmax_y = tf.map_fn(lambda img: tf.cast(tf.argmax(img), tf.float64) % tf.cast(img_len, dtype=tf.float64), tf.cast(flat_img, dtype=tf.float64))\n",
    "\n",
    "# #     res = tf.stack([argmax_x, argmax_y, tf.zeros([batch_len], dtype=tf.float64)], axis = 1, name= \"pred_crds\")    \n",
    "    \n",
    "# #     pred_crds = tf.cast(res, dtype = tf.int64, name= \"predicted_crds\")    \n",
    "    \n",
    "    \n",
    "#     ### some more performance measures\n",
    "#     mask_correct_prediction = tf.equal(y_pred, mask_y_true)\n",
    "#     mask_accuracy = tf.reduce_mean(tf.cast(mask_correct_prediction, tf.float32))        \n",
    "\n",
    "# # name_scope = 'mask_sd'\n",
    "# # with tf.name_scope(name_scope):\n",
    "\n",
    "# #     input_sd = tf.cast(items, dtype = tf.float32, name=layer_name + \"/input_sd\")\n",
    "    \n",
    "# #     layer1_conv1, weights_conv1 = new_conv_layer(input=input_sd,\n",
    "# #                                                 num_input_channels=img_shape[2],\n",
    "# #                                                 filter_size=filter_size1,\n",
    "# #                                                 num_filters=num_filters1,\n",
    "# #                                                  name_scope = 'mask_sd_graph',\n",
    "# #                                                  layer_name = 'conv1',\n",
    "# #                                                 use_pooling=True)\n",
    "\n",
    "# #     layer2_conv2, weights_conv2 = new_conv_layer(input=layer1_conv1,\n",
    "# #                                                 num_input_channels=num_filters1,\n",
    "# #                                                 filter_size=filter_size2,\n",
    "# #                                                 num_filters=num_filters2,\n",
    "# #                                                  name_scope = 'mask_sd_graph',\n",
    "# #                                                  layer_name = 'conv2',\n",
    "# #                                                 use_pooling=True)\n",
    "\n",
    "# #     layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "# #                                                num_input_channels=num_filters2,\n",
    "# #                                                filter_size=filter_size3,\n",
    "# #                                                num_filters=num_filters3,\n",
    "# #                                                  name_scope = 'mask_sd_graph',\n",
    "# #                                                  layer_name = 'conv3',\n",
    "# #                                                use_pooling=True)\n",
    "\n",
    "# #     layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "# #                                                num_input_channels=num_filters3,\n",
    "# #                                                filter_size=filter_size4,\n",
    "# #                                                num_filters=num_filters4,\n",
    "# #                                                  name_scope = 'mask_sd_graph',\n",
    "# #                                                  layer_name = 'conv4',\n",
    "# #                                                use_pooling=True)\n",
    "\n",
    "# #     layer_flat, num_features = flatten_layer(layer4_conv4)       \n",
    "\n",
    "# #     layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "# #                      num_inputs=num_features,\n",
    "# #                      num_outputs=fc_size,\n",
    "# #                      name_scope = 'mask_sd_graph',\n",
    "# #                      layer_name = 'fc1',\n",
    "# #                      use_relu=True)\n",
    "\n",
    "# #     layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "# #                              num_inputs=fc_size,\n",
    "# #                              num_outputs=fc_size,\n",
    "# #                              name_scope = 'mask_sd_graph',\n",
    "# #                              layer_name = 'fc2',\n",
    "# #                              use_relu=False)\n",
    "\n",
    "# #     layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "# #                              num_inputs=fc_size,\n",
    "# #                              num_outputs=fc_size,\n",
    "# #                              name_scope = 'mask_sd_graph',\n",
    "# #                              layer_name = 'fc3',\n",
    "# #                              use_relu=False)\n",
    "\n",
    "# #     layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "# #                              num_inputs=fc_size,\n",
    "# #                              num_outputs=num_classes,\n",
    "# #                              name_scope = 'mask_sd_graph',\n",
    "# #                              layer_name = 'fc4',\n",
    "# #                              use_relu=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# #     sd_y_pred = tf.nn.softmax(layer_fc4, name=layer_name + \"/sd_y_pred\")\n",
    "# #     sd_y_pred_cls = tf.argmax(sd_y_pred, axis=1)\n",
    "    \n",
    "# #     sd_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=sd_y_pred, labels=sd_y_true)\n",
    "    \n",
    "# #     train_op1 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(mask_loss)\n",
    "# #     train_op2 = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(sd_loss)\n",
    "# #     final_train_op = tf.group(train_op1, train_op2)    \n",
    "\n",
    "# #     sd_cost = tf.reduce_mean(sd_loss)\n",
    "    \n",
    "# #     sd_correct_prediction = tf.equal(sd_y_pred_cls, sd_y_true_cls)\n",
    "# #     sd_accuracy = tf.reduce_mean(tf.cast(sd_correct_prediction, tf.float32))        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
