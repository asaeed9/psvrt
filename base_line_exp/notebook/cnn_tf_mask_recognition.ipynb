{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2, os, math, time\n",
    "from datetime import timedelta\n",
    "from sklearn.utils import shuffle\n",
    "from numpy.lib.stride_tricks import as_strided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Configuration\n",
    "\"\"\"\n",
    "Data Configurations/Paths\n",
    "\"\"\"\n",
    "img_dir=\"../../original_images/SD\"\n",
    "base_model = 'SD/sd_01_.ckpt'\n",
    "model_20000 = 'SD/sd_20000.ckpt'\n",
    "model_30000 = 'SD/sd_30000.ckpt'\n",
    "model_40000 = 'SD/sd_40000.ckpt'\n",
    "\n",
    "connected_model = 'SD/sd_conected.ckpt'\n",
    "\n",
    "##\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 4          # Convolution filters are 4 x 4 pixels.\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters2 = 32         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size3 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters3 = 64         # There are 64 of these filters.\n",
    "\n",
    "# Convolutional Layer 4.\n",
    "filter_size4 = 2          # Convolution filters are 2 x 2 pixels.\n",
    "num_filters4 = 128         # There are 128 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 2000             # Number of neurons in fully-connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label_dimensions(labels):\n",
    "    label_temp = np.zeros((len(labels), 2))\n",
    "    \n",
    "    for idx in range(0, len(labels)):\n",
    "        if labels[idx] == 1:\n",
    "            label_temp[idx][1] = 1\n",
    "        else:\n",
    "            label_temp[idx][0] = 1\n",
    "    \n",
    "    return label_temp\n",
    "\n",
    "def load_data(img_dir):\n",
    "        list_of_imgs = []\n",
    "        list_of_labels = []\n",
    "        for img in os.listdir(img_dir):\n",
    "            img_path = os.path.join(img_dir, img)\n",
    "            for img_label in os.listdir(img_path):\n",
    "                img_data = os.path.join(img_path, img_label)\n",
    "                if img_label == \"img\":\n",
    "#                     print(img_data + \"/img.png\")\n",
    "                    list_of_imgs.append(img_data + \"/img.png\")\n",
    "                else:\n",
    "                        list_of_labels.append([os.path.join(img_data, label) for label in os.listdir(img_data)])\n",
    "\n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "        data_labels = np.array(list_of_labels)\n",
    "\n",
    "        return data_imgs, data_labels\n",
    "\n",
    "\n",
    "# def load_data(img_dir, label):\n",
    "#         list_of_imgs = []\n",
    "#         list_of_labels = []\n",
    "#         for img in os.listdir(img_dir):\n",
    "#                 img = os.path.join(img_dir, img)\n",
    "# #                 print(img)\n",
    "#                 if not img.endswith(\".png\"):\n",
    "#                         continue\n",
    "\n",
    "#                 list_of_imgs.append(img)\n",
    "#                 list_of_labels.append(label)\n",
    "#         data_labels = np.asarray(list_of_labels, dtype=np.int32)\n",
    "#         data_imgs = np.array(list_of_imgs)\n",
    "# #         data_labels = np.array(list_of_labels)\n",
    "#         return data_imgs, data_labels\n",
    "    \n",
    "def get_batch_images(data, label):\n",
    "        list_of_imgs = []\n",
    "        list_of_labels = []\n",
    "        for img, lbl in zip(data, label):\n",
    "            orig_img = cv2.imread(img)\n",
    "            #only first image as a label\n",
    "            orig_lbl = cv2.imread(lbl[0])\n",
    "            if orig_img is None or orig_lbl is None:\n",
    "                    print (\"Unable to read image{} or {}\".format(img, lbl))\n",
    "                    continue\n",
    "            \n",
    "            flattened_img = orig_img.flatten()\n",
    "            flattened_lbl = orig_lbl.flatten()\n",
    "\n",
    "            list_of_imgs.append(np.asarray(flattened_img, dtype=np.float32))\n",
    "            list_of_labels.append(np.asarray(flattened_lbl, dtype=np.float32))\n",
    "\n",
    "        data_labels = np.array(list_of_labels)\n",
    "        data_imgs = np.array(list_of_imgs)\n",
    "\n",
    "        \n",
    "        return data_imgs, data_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Batch Own Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that images are 60 pixels in each dimension.\n",
    "# img_size = 8 * 4\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = 4 * 4\n",
    "\n",
    "# Number of colour channels for the images: 3 channel for RGB.\n",
    "num_channels = 3\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (4, 4, num_channels)\n",
    "\n",
    "\n",
    "# Number of classes, one class for same or different image\n",
    "num_classes = 2*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getActivations(layer,stimuli):\n",
    "    units = sess.run(layer,feed_dict={x:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})\n",
    "    plotNNFilter(units)\n",
    "    \n",
    "def plotNNFilter(units):\n",
    "    filters = units.shape[3]\n",
    "    plt.figure(1, figsize=(20,20))\n",
    "    n_columns = 6\n",
    "    n_rows = math.ceil(filters / n_columns) + 1\n",
    "    for i in range(filters):\n",
    "        plt.subplot(n_rows, n_columns, i+1)\n",
    "        plt.title('Filter ' + str(i))\n",
    "        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n",
    "        \n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(cv2.imread(images[i]).flatten().reshape((8,4, 3)), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def generate_size_graph(fig_no, training_size, accuracy, loss, patch_only,patch_conv, start_size, end_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(training_size,accuracy)\n",
    "    plt.plot(training_size,loss)\n",
    "    plt.plot(training_size, patch_only)\n",
    "    plt.plot(training_size, patch_conv)\n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Size vs Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['SD Original Accuracy','SR Accuracy', 'SD Patch Accuracy', 'SD Patch Conv'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(path + '/batch_graphs/' +  str(start_size) + '_' + str(end_size) + '.jpg') \n",
    "        \n",
    "def generate_graph(fig_no, epochs, train, val, label, train_title, val_title, train_size):\n",
    "    plt.figure(fig_no,figsize=(7,5))\n",
    "    plt.plot(epochs,train)\n",
    "    plt.plot(epochs,val)\n",
    "    plt.xlabel('num of Epochs')\n",
    "    plt.ylabel(label)\n",
    "    plt.title(train_title + ' vs ' + val_title + '( Samples:' + str(train_size) + ')')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['train','val'])\n",
    "    plt.style.use(['classic'])\n",
    "    plt.show()\n",
    "    plt.savefig(results_path + '/batch_graphs/' +  label + '_' + str(train_size) + '.jpg')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Helper Functions for TF Graph Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(shape, layer_name):\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape), name=layer_name+'_W')\n",
    "\n",
    "def new_bias(length, layer_name):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]), name=layer_name+'_b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   name_scope,\n",
    "                   layer_name='',\n",
    "                   use_pooling=True):\n",
    "\n",
    "    with tf.name_scope(name_scope):\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "        weights = new_weights(shape, layer_name)\n",
    "        biases = new_bias(num_filters, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.nn.conv2d(input=input, filter=weights, strides=[1,1,1,1], padding='SAME'), biases, name=layer_name)\n",
    "\n",
    "        if use_pooling:\n",
    "            layer = tf.nn.max_pool(value=layer,\n",
    "                                   ksize=[1, 3, 3, 1],\n",
    "                                   strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME', name=layer_name+'_max')\n",
    "        layer = tf.nn.relu(layer, name=layer_name+'_activation')\n",
    "\n",
    "    return layer, weights\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "    \n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,\n",
    "                num_inputs,\n",
    "                num_outputs,\n",
    "                name_scope,\n",
    "                layer_name='',\n",
    "                use_relu=True):\n",
    "    \n",
    "    with tf.name_scope(name_scope):\n",
    "        weights = new_weights([num_inputs, num_outputs], layer_name)\n",
    "        biases = new_bias(num_outputs, layer_name)\n",
    "\n",
    "        layer = tf.add(tf.matmul(input, weights),biases,name=layer_name)\n",
    "    #     layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "        if use_relu:\n",
    "            layer = tf.nn.relu(layer, layer_name+'_activation')\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(4), Dimension(4), Dimension(3)]),\n",
       " <tf.Tensor 'y_true_1:0' shape=(?, 300) dtype=float32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat*num_channels], name='x')\n",
    "x_image = tf.reshape(x, [-1, 4, 4, num_channels])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes * num_channels], name='y_true')\n",
    "# y_true_cls = tf.argmax(y_true, axis=1)\n",
    "y_true_cls = tf.placeholder(tf.float32, shape=[None, num_classes * num_channels], name='y_true_cls')\n",
    "x_image.shape, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_conv1, weights_conv1 = new_conv_layer(input=x_image,\n",
    "                                            num_input_channels=num_channels,\n",
    "                                            filter_size=filter_size1,\n",
    "                                            num_filters=num_filters1,\n",
    "                                             name_scope = 'cv',\n",
    "                                             layer_name='conv1',\n",
    "                                            use_pooling=True)\n",
    "\n",
    "layer2_conv2, weights_conv2 =  new_conv_layer(input=layer1_conv1,\n",
    "                                           num_input_channels=num_filters1,\n",
    "                                           filter_size=filter_size2,\n",
    "                                           num_filters=num_filters2,\n",
    "                                             name_scope = 'cv',\n",
    "                                             layer_name='conv2',\n",
    "                                           use_pooling=True)\n",
    "\n",
    "layer3_conv3, weights_conv3 =  new_conv_layer(input=layer2_conv2,\n",
    "                                           num_input_channels=num_filters2,\n",
    "                                           filter_size=filter_size3,\n",
    "                                           num_filters=num_filters3,\n",
    "                                             name_scope = 'cv',\n",
    "                                             layer_name='conv3',\n",
    "                                           use_pooling=True)\n",
    "\n",
    "layer4_conv4, weights_conv4 =  new_conv_layer(input=layer3_conv3,\n",
    "                                           num_input_channels=num_filters3,\n",
    "                                           filter_size=filter_size4,\n",
    "                                           num_filters=num_filters4,\n",
    "                                             name_scope = 'cv',\n",
    "                                             layer_name='conv4',\n",
    "                                           use_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_flat, num_features = flatten_layer(layer4_conv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features,\n",
    "                         num_outputs=fc_size+2,\n",
    "                         name_scope = 'fc',\n",
    "                         layer_name = 'fc1',\n",
    "                         use_relu=True)\n",
    "\n",
    "\n",
    "layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size+2,\n",
    "                         num_outputs=fc_size+4,\n",
    "                         name_scope = 'fc',\n",
    "                         layer_name = 'fc2',\n",
    "                         use_relu=False)\n",
    "\n",
    "layer_fc3 = new_fc_layer(input=layer_fc2,\n",
    "                         num_inputs=fc_size+4,\n",
    "                         num_outputs=num_classes * num_channels,\n",
    "                         name_scope = 'fc',\n",
    "                         layer_name = 'fc3',\n",
    "                         use_relu=False)\n",
    "\n",
    "layer_fc4 = new_fc_layer(input=layer_fc3,\n",
    "                         num_inputs=num_classes * num_channels,\n",
    "                         num_outputs=num_classes * num_channels,\n",
    "                         name_scope = 'fc',\n",
    "                         layer_name = 'fc4',\n",
    "                         use_relu=False)\n",
    "\n",
    "y_pred_cls = layer_fc4\n",
    "y_pred = layer_fc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(y_true - y_pred_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "\n",
    "## some more performance measures\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Tensorflow on Defined Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "train_data, train_labels = load_data(img_dir)\n",
    "train_batch_size = 64\n",
    "\n",
    "def optimize(num_iterations, save_model=True,save_name=base_model,restore_model=False,restore_name=None):\n",
    "    total_iterations = 0\n",
    "    done_train_imgs = 0\n",
    "    start_time = time.time()\n",
    "    start_batch=0\n",
    "    end_batch = train_batch_size\n",
    "    plot_accuracy=[]\n",
    "    plot_training_size=[]\n",
    "    \n",
    "    #to save the model\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if restore_model==True:\n",
    "        if restore_name==None:\n",
    "            print(\"No model file specified\")\n",
    "            return\n",
    "        else:\n",
    "            saver.restore(session,restore_name)\n",
    "            \n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        total_iterations = 0\n",
    "        start_batch=0\n",
    "        end_batch = train_batch_size\n",
    "#         train_data, train_labels = load_data(img_dir)\n",
    "        while total_iterations < 30:\n",
    "            train = train_data[start_batch:end_batch]\n",
    "            labels = train_labels[start_batch:end_batch]\n",
    "            train, labels = get_batch_images(train, labels)\n",
    "            if not len(train) and not len(labels):\n",
    "                print(\"All images have been processed.\")\n",
    "                break;\n",
    "\n",
    "            x_batch, y_true_batch = next_batch(train_batch_size, train, labels)\n",
    "    #         x_batch, y_true_batch = train_data[done_train_images:done_train_imgs+train_batch_size]\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                       y_true: y_true_batch}\n",
    "            \n",
    "            session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "    #         if total_iterations%1000==0:    \n",
    "            acc,co = session.run([accuracy, cost], feed_dict=feed_dict_train)\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}, Loss: {2:>.4f}\"\n",
    "            print(msg.format(total_iterations + 1, acc, co))\n",
    "            \n",
    "            plot_accuracy.append(acc)\n",
    "            plot_training_size.append((total_iterations + 1) * 64)\n",
    "\n",
    "                # Update the total number of iterations performed.\n",
    "    #         done_train_imgs+=train_batch_size\n",
    "            start_batch += train_batch_size\n",
    "            end_batch += train_batch_size\n",
    "            total_iterations +=1\n",
    "\n",
    "        if save_model==True:\n",
    "            if save_name==None:\n",
    "                print(\"No model specified, model not being saved\")\n",
    "                return\n",
    "            else:\n",
    "                save_path = saver.save(session, save_name)\n",
    "                print(\"Model saved in file: %s\" % save_name)\n",
    "        \n",
    "#         total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))  \n",
    "    print(plot_accuracy)\n",
    "    print(plot_training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_model = True\n",
    "save_name = model_30000\n",
    "restore_model=False\n",
    "restore_name=model_30000\n",
    "\n",
    "optimize(num_iterations=30, save_model=True,save_name=model_30000,restore_model=False,restore_name=model_30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(tensor):\n",
    "    return tf.div(\n",
    "   tf.subtract(\n",
    "      tensor, \n",
    "      tf.reduce_min(tensor)\n",
    "   ), \n",
    "   tf.subtract(\n",
    "      tf.reduce_max(tensor), \n",
    "      tf.reduce_min(tensor)\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_see_layer(ix, model_name=None, var_name=None):\n",
    "    with tf.Session('', tf.Graph()) as s:\n",
    "        with s.graph.as_default():\n",
    "            if ((model_name != None) and var_name != None):\n",
    "                saver = tf.train.import_meta_graph(model_name+\".meta\")\n",
    "                saver.restore(s, model_name)\n",
    "                fd = {'x:0':ix}\n",
    "                var_name=var_name+\":0\"\n",
    "                    \n",
    "                result = 0\n",
    "                result = s.run(var_name, feed_dict=fd)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_data(img_dir)\n",
    "train = train_data[0:64]\n",
    "labels = train_labels[0:64]\n",
    "train, labels = get_batch_images(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 255. 255.\n",
      "  255. 255. 255. 255.   0.   0.   0.   0.   0.   0. 255. 255. 255. 255.\n",
      "  255. 255.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "img_x = train[0:1, :]#np.expand_dims(train[0], axis=0).shape\n",
    "lbl_x = labels[0:1, :]\n",
    "\n",
    "# print(img_x)\n",
    "print(lbl_x)\n",
    "\n",
    "# output_cl1 = restore_see_layer(ix=img_x,model_name=model_30000,var_name='fc_3/fc4')\n",
    "# output_fc4_w = restore_see_layer(ix=img_x,model_name=model_30000,var_name='fc_3/fc4_W')\n",
    "# output_fc3_w = restore_see_layer(ix=img_x,model_name=model_30000,var_name='fc_2/fc3_W')\n",
    "\n",
    "# print(output_cl1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# np.set_printoptions(suppress=True)\n",
    "# # print('original image:', original_image)\n",
    "# # print(np.array(output_cl1, dtype=np.int64))\n",
    "\n",
    "# # print(output_cl1)\n",
    "\n",
    "lbl_x[np.where(lbl_x < 100)] = 0.\n",
    "lbl_x[np.where(lbl_x > 100)] = 1.\n",
    "\n",
    "# print(lbl_x)\n",
    "# print(output_cl1)\n",
    "# print(output_fc4_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(29,)\n",
      "[1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "(24,)\n",
      "[6. 5. 4. 3. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(24,)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[255. 255. 255.   0. 255. 255.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([255., 255., 255.,   0., 255., 255.], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess2 = tf.Session()\n",
    "patch_size = 6\n",
    "m = np.squeeze(lbl_x)[24:48]\n",
    "\n",
    "# kernel = tf.constant(1, shape=(patch_size,), dtype=tf.float32)\n",
    "\n",
    "# data   = tf.reshape(i, [1, int(i.shape[1]), 1], name='data')\n",
    "# kernel = tf.reshape(kernel, [int(kernel.shape[0]), 1, 1], name='kernel')\n",
    "\n",
    "# res = tf.squeeze(tf.nn.conv1d(data, kernel, 1, 'SAME'))\n",
    "\n",
    "\n",
    "# k = tf.ones([patch_size], dtype=tf.float32, name='k')\n",
    "# beta = tf.Variable(tf.random_normal([10,10], dtype=tf.float32, seed=2))\n",
    "\n",
    "mask = tf.Variable(m, dtype=tf.float32, name='mask')\n",
    "k = tf.ones([patch_size], dtype=tf.float32, name='k')\n",
    "\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "print(mask.eval(session=sess2))\n",
    "paddings = tf.constant([[0, patch_size-1]])\n",
    "padded_mask = tf.pad(tf.squeeze(mask), paddings, \"CONSTANT\")\n",
    "print(padded_mask.get_shape())\n",
    "print(padded_mask.eval(session=sess2))\n",
    "padded_mask   = tf.reshape(padded_mask, [1, int(padded_mask.shape[0]), 1], name='padded_mask')\n",
    "kernel = tf.reshape(k, [int(k.shape[0]), 1, 1], name='kernel')\n",
    "\n",
    "res = tf.squeeze(tf.nn.conv1d(padded_mask, kernel, 1, 'VALID'))\n",
    "print(res.get_shape())\n",
    "print(res.eval(session=sess2))\n",
    "\n",
    "\n",
    "# logits = tf.matmul(tf.expand_dims(res, 0), output_fc4_w)\n",
    "softmax_logis = tf.nn.softmax(tf.expand_dims(res, 0))\n",
    "\n",
    "weights = tf.squeeze(tf.cast(tf.where(\n",
    "        tf.equal(tf.reduce_max(softmax_logis, axis=1, keepdims=True), softmax_logis), \n",
    "        tf.constant(1, shape=softmax_logis.shape), \n",
    "        tf.constant(0, shape=softmax_logis.shape)\n",
    "    ), dtype=tf.float32))\n",
    "\n",
    "print(weights.get_shape())\n",
    "print(weights.eval(session=sess2))\n",
    "\n",
    "# weights_var = tf.Variable(weights, name=\"weights_var\")\n",
    "# indices = tf.squeeze(tf.where(tf.greater(weights, 0)))\n",
    "# sess2.run(tf.global_variables_initializer())\n",
    "\n",
    "# valuesofindices = np.delete(indices.eval(session=sess2),\n",
    "#                             0)\n",
    "\n",
    "# updated_weights = tf.scatter_update(weights_var,\n",
    "#                            valuesofindices,\n",
    "#                            tf.tile(tf.constant([0],\n",
    "#                                    tf.float32),\n",
    "#                            valuesofindices.shape))\n",
    "\n",
    "# print(updated_weights.eval(session=sess2))\n",
    "\n",
    "end_range = 19\n",
    "img_x_t = tf.convert_to_tensor(np.squeeze(np.array(img_x))[24:48])\n",
    "xm = tf.map_fn(lambda i: img_x_t[i:i+end_range], tf.range(patch_size), dtype=tf.float32)\n",
    "patch = tf.reduce_sum(tf.matmul(xm, tf.expand_dims(weights[0:end_range], 1)), axis=1)\n",
    "print(img_x_t.eval(session=sess2))\n",
    "patch.eval(session=sess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_output(iNp,depth_filter_to_see=0,cmap=\"gray\",figsize=(4,4)):\n",
    "    img_x = iNp[0,:,:,:]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.imshow(img_x, interpolation='none', aspect='auto')\n",
    "#     plt.colorbar(img_x, orientation='horizontal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAD8CAYAAAC/+/tYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADIBJREFUeJzt3XGoZOV9xvHvU93YUtNoXcFld9UUJSSERuOyNQhFTASV4BYiVP9INBhuCbExpYGGFlIaKJj+kdA0IUVUqiEkBk3TbbCELW6aBKrxuqzGXWuyFYK7kbrRZM2SYLj21z/mKDeTu97bd86dM3P3+4HhnjPz7rzvsMuzc885M0+qCklq8RtDL0DS/DJAJDUzQCQ1M0AkNTNAJDUzQCQ1myhAkvxukj1JftD9PPME415Osr+77Z5kTkmzI5NcB5Lk74AXquq2JB8Dzqyqv1hh3PGqOn2CdUqaQZMGyFPA5VX1bJItwDer6k0rjDNApA1o0gD5aVWd0W0H+Mkr+2PjloD9wBJwW1V97QTPtwAsdLuXNC9M0qR+XFVnrzbo1NUGJPl34JwVHvqr5TtVVUlOlEbnVdWRJL8HPJjke1X13+ODqup24PZuXq+xl4bzw7UMWjVAqupdJ3osyf8k2bLsV5jnTvAcR7qfTyf5JnAx8GsBImm+THoadzdwY7d9I/Av4wOSnJnktG57M3AZcHDCeSXNgEkD5DbgyiQ/AN7V7ZNkR5I7ujFvBhaTPAbsZXQMxACRNoCJDqKuJ4+BSIN6tKp2rDbIK1ElNTNAJDUzQCQ1M0AkNTNAJDUzQCQ1M0AkNTNAJDUzQCQ1M0AkNTNAJDUzQCQ1M0AkNTNAJDUzQCQ1M0AkNTNAJDUzQCQ16yVAklyV5Kkkh7qGuvHHT0tyb/f4w0nO72NeScOaOECSnAJ8DrgaeAtwQ5K3jA27mVHp1AXAp4FPTjqvpOH18Q5kJ3Coqp6uql8CXwZ2jY3ZBdzdbd8HvLNrspM0x/oIkK3AM8v2D3f3rTimqpaAY8BZPcwtaUCrNtNN01g3rqQZ18c7kCPA9mX727r7VhyT5FTgDcDz409UVbdX1Y619FFIGl4fAfIIcGGSNyZ5HXA9o8rL5ZZXYF4HPFiz2mglac0m/hWmqpaS3AJ8AzgFuKuqDiT5BLBYVbuBO4EvJDkEvMAoZCTNOastJa3EaktJ68sAkdTMAJHUzACR1MwAkdTMAJHUzACR1MwAkdTMAJHUzACR1MwAkdTMAJHUzACR1MwAkdTMAJHUzACR1MwAkdTMAJHUzACR1Gxa3bg3JTmaZH93+0Af80oa1sTfyr6sG/dKRq10jyTZXVUHx4beW1W3TDqfpNnRRzPdq924AEle6cYdD5D/l0sugcXFHlY3ozb+l85bfXwymFY3LsB7kjye5L4k21d4nCQLSRaTLB492sPKJK2raR1E/Vfg/Kr6fWAPcPdKg5ZXW5599pRWJqnZVLpxq+r5qnqp270DuKSHeSUNbCrduEm2LNu9Fniyh3klDWxa3bgfTnItsMSoG/emSeeVNLyZ7cbdsSPlWZh55lmYOWc3rqT1ZYBIamaASGpmgEhqZoBIamaASGpmgEhqZoBIamaASGpmgEhqZoBIamaASGpmgEhqZoBIamaASGpmgEhqZoBIamaASGrWV7XlXUmeS/LECR5Pks901ZePJ3l7H/NKGlZf70D+CbjqNR6/Griwuy0An+9pXkkD6iVAqupbjL5t/UR2AffUyEPAGWNVD5Lm0LSOgayp/tJqS2m+zNRBVKstpfkyrQBZtf5S0vyZVoDsBt7XnY25FDhWVc9OaW5J62TiakuAJF8CLgc2JzkM/DWwCaCq/hF4ALgGOAT8HHh/H/NKGlYvAVJVN6zyeAEf6mMuSbNjpg6iSpovBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGbTqra8PMmxJPu728f7mFfSsHr5TlRG1ZafBe55jTHfrqp39zSfpBkwrWpLSRtQX+9A1uIdSR4DfgR8tKoOjA9IssCofJtzzz0X+OEUlzdlNfQC1lmGXoCmYVoHUfcB51XV24B/AL620qBfrba021KadVMJkKp6saqOd9sPAJuSbJ7G3JLWz1QCJMk5SdJt7+zmfX4ac0taP9OqtrwO+GCSJeAXwPVdW52kOTatasvPMjrNK2kD8UpUSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzSYOkCTbk+xNcjDJgSS3rjAmST6T5FCSx5O8fdJ5JQ2vj+9EXQL+vKr2JXk98GiSPVV1cNmYq4ELu9sfAJ/vfkqaYxO/A6mqZ6tqX7f9M+BJYOvYsF3APTXyEHBGki2Tzi1pWL0eA0lyPnAx8PDYQ1uBZ5btH+bXQ4YkC0kWkywePXq0z6VJWge9BUiS04H7gY9U1Ystz2G1pTRfegmQJJsYhccXq+qrKww5Amxftr+tu0/SHOvjLEyAO4Enq+pTJxi2G3hfdzbmUuBYVT076dyShtXHWZjLgPcC30uyv7vvL4Fz4dVqyweAa4BDwM+B9/cwr6SBTRwgVfUdIKuMKeBDk84labZ4JaqkZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRmBoikZgaIpGYGiKRm06q2vDzJsST7u9vHJ51X0vCmVW0J8O2qencP80maEdOqtpS0AfXxDuRVr1FtCfCOJI8BPwI+WlUHVvjzC8DCsv0+lyepZxk1LvTwRKNqy/8A/na8nS7J7wD/W1XHk1wD/H1VXbjK8/WzMEktHq2qHasNmkq1ZVW9WFXHu+0HgE1JNvcxt6ThTKXaMsk53TiS7OzmfX7SuSUNa1rVltcBH0yyBPwCuL76+t1J0mB6OwbSN4+BSIOa3jEQSScnA0RSMwNEUjMDRFIzA0RSMwNEUjMDRFIzA0RSMwNEUjMDRFIzA0RSMwNEUjMDRFIzA0RSMwNEUjMDRFIzA0RSMwNEUrM+vlT5N5N8N8ljXbXl36ww5rQk9yY5lOThrj9G0pzr4x3IS8AVVfU24CLgqiSXjo25GfhJVV0AfBr4ZA/zShpYH9WW9UrnC7Cpu41/IfIu4O5u+z7gnbF2Tpp7fRVLndJVOjwH7Kmq8WrLrcAzAFW1BBwDzupjbknD6SVAqurlqroI2AbsTPLWludJspBkMcliH+uStL56PQtTVT8F9gJXjT10BNgOkORU4A2s0ExXVbdX1Y619FFIGl4fZ2HOTnJGt/1bwJXAf40N2w3c2G1fBzxoM500//qottwC3J3kFEaB9JWq+nqSTwCLVbWbUXfuF5IcAl4Aru9hXkkDs9pS0kqstpS0vgwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzQwQSc0MEEnNDBBJzabVjXtTkqNJ9ne3D0w6r6Th9fGt7K904x5Psgn4TpJ/q6qHxsbdW1W39DCfpBkxcYB0/S6rdeNK2oD6eAdC1wnzKHAB8LkVunEB3pPkD4HvA39WVc+s8DwLwEK3exx4qo/1rdFm4MdTnG/afH3zbdqv77y1DOq1F6ZrqPtn4E+r6oll958FHK+ql5L8CfDHVXVFbxP3IMniRq7U9PXNt1l9fVPpxq2q56vqpW73DuCSPueVNIypdOMm2bJs91rgyUnnlTS8aXXjfjjJtcASo27cm3qYt2+3D72Adebrm28z+fpmthtX0uzzSlRJzQwQSc0MECDJVUmeSnIoyceGXk+fktyV5LkkT6w+ev4k2Z5kb5KD3Ucpbh16TX1Zy8dEhnbSHwPpDv5+n9HZo8PAI8ANVXVw0IX1pLt47zhwT1W9dej19K07w7elqvYleT2jCxr/aCP8/SUJ8NvLPyYC3LrCx0QG4zsQ2Akcqqqnq+qXwJeBXQOvqTdV9S1GZ742pKp6tqr2dds/Y3SJwNZhV9WPGpnpj4kYIKN/bMsvqz/MBvkHeLJJcj5wMbDSRynmUpJTkuwHngP2nOBjIoMxQLQhJDkduB/4SFW9OPR6+lJVL1fVRcA2YGeSmfo11ACBI8D2Zfvbuvs0J7rjA/cDX6yqrw69nvVwoo+JDM0AGR00vTDJG5O8Drge2D3wmrRG3YHGO4Enq+pTQ6+nT2v5mMjQTvoAqaol4BbgG4wOwH2lqg4Mu6r+JPkS8J/Am5IcTnLz0Gvq2WXAe4Erln3j3TVDL6onW4C9SR5n9B/dnqr6+sBr+hUn/WlcSe1O+ncgktoZIJKaGSCSmhkgkpoZIJKaGSCSmhkgkpr9H1X+6Tgvp6lGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_image = np.reshape(img_x, (1, 4, 4, 3))\n",
    "# predicted_image = np.reshape(output_cl1, (1, 4, 4, 3))\n",
    "visual_image = np.reshape(patch.eval(session=sess2), (1, 1, 2, 3))\n",
    "visual_label = np.reshape(lbl_x, (1, 4, 4, 3))\n",
    "\n",
    "# visual_image_conn = np.reshape(output_fully_connected, (1, 120, 120, 3))\n",
    "# visual_label_conn = np.reshape(lbl_x, (1, 120, 120, 3))\n",
    "see_output(original_image)\n",
    "see_output(visual_label)\n",
    "# see_output(predicted_image)\n",
    "see_output(visual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARQAAAD8CAYAAAC2EFsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADC1JREFUeJzt3W+onvV9x/H3R0Psg9rWf8Tgn0ZZ2EjHaO1NsB3rypqC+iARWjploxEcWRFhMPYgENgD+2DasT+MOrbgCmkfVFthM1srTmNLnyzOZLNaK2mibJgsmv4ZblJW5/bdg3M5Ts/uk+Tk/ubc13HvFxzOdd3X775/v4uD71z3dY7npKqQpA4XzHsBkt4+DIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbdbNewHLyeUpNs17FTpXHzw87xVoFofhB1V1xUqfN9qgsAk4NO9F6FwdyrxXoFkE/vlcnudbHkltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2swUlCSXJnk8ydHh8yWnGfuuJMeTfH6WOSWN16xXKLuBA1W1GTgw7C/ns8C3ZpxP0ojNGpQdwL5hex9w67RBST4IbAD+dsb5JI3YrEHZUFUnh+1XWIjGT0lyAfAHwO/MOJekkTvjL6lO8gRw5ZRDexbvVFUlqSnj7gK+XlXHk9P/5uIku4BdAFx7ppVJGpszBqWqti13LMmrSTZW1ckkG4FTU4Z9CPilJHcB7wTWJ3m9qv7P/Zaq2gvsBchkapwkjdisf0ZjP7ATuHf4/MjSAVX1a29tJ7kDmEyLiaS1b9Z7KPcCH09yFNg27JNkkuSBWRcnaW1J1TjfWWSS8g99rV3lH/pa0wKHq2qy0uf5k7KS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktoYFEltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktoYFEltDIqkNjMFJcmlSR5PcnT4fMmUMe9P8ndJnk/ybJJfnWVOSeM16xXKbuBAVW0GDgz7S/0Y+HRVvQ+4CfjjJO+ZcV5JIzRrUHYA+4btfcCtSwdU1feq6uiw/S/AKeCKGeeVNEKzBmVDVZ0ctl8BNpxucJKtwHrgxRnnlTRC6840IMkTwJVTDu1ZvFNVlaRO8zobgS8BO6vqv5cZswvYBcC1Z1qZpLE5Y1Cqattyx5K8mmRjVZ0cgnFqmXHvAr4G7Kmqg6eZay+wFyCT5eMkaZxmfcuzH9g5bO8EHlk6IMl64C+BL1bVwzPOJ2nEZg3KvcDHkxwFtg37JJkkeWAY8yngI8AdSZ4ZPt4/47ySRihV43xnkUmKQ/Nehc5VZd4r0CwCh6tqstLn+ZOyktoYFEltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktoYFEltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbVqCkuSmJEeSHEuye8rxi5I8NBx/KsmmjnkljcvMQUlyIXA/cDOwBbg9yZYlw+4E/rWqfgb4I+C+WeeVND4dVyhbgWNV9VJVvQE8COxYMmYHsG/Yfhj4WJI0zC1pRDqCchXw8qL948NjU8dU1ZvAa8BlDXNLGpFR3ZRNsivJoSSH+P68VyNppTqCcgK4ZtH+1cNjU8ckWQe8G/jh0heqqr1VNamqCVc0rEzSquoIytPA5iTXJVkP3AbsXzJmP7Bz2P4k8GRVVcPckkZk3awvUFVvJrkbeAy4EPhCVT2f5B7gUFXtB/4C+FKSY8CPWIiOpLeZjPVCIZMUh+a9Cp2r8nt4a1rgcFVNVvq8Ud2UlbS2GRRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktoYFEltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2LUFJclOSI0mOJdk95fhvJ/lukmeTHEjy3o55JY3LzEFJciFwP3AzsAW4PcmWJcP+EZhU1S8ADwOfm3VeSePTcYWyFThWVS9V1RvAg8COxQOq6htV9eNh9yBwdcO8kkamIyhXAS8v2j8+PLacO4FHG+aVNDLrVnOyJL8OTIBfXub4LmAXANeu3rok9ei4QjkBXLNo/+rhsZ+SZBuwB9heVT+Z9kJVtbeqJlU14YqGlUlaVR1BeRrYnOS6JOuB24D9iwck+QDw5yzE5FTDnJJGaOagVNWbwN3AY8ALwFeq6vkk9yTZPgz7feCdwFeTPJNk/zIvJ2kNS1XNew1TZZLi0LxXoXNVmfcKNIvA4aqarPR5/qSspDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktoYFEltDIqkNgZFUhuDIqmNQZHUxqBIamNQJLUxKJLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSm5agJLkpyZEkx5LsPs24TySpJJOOeSWNy8xBSXIhcD9wM7AFuD3JlinjLgZ+C3hq1jkljVPHFcpW4FhVvVRVbwAPAjumjPsscB/wHw1zShqhjqBcBby8aP/48Nj/SnIDcE1Vfa1hPkkjte58T5DkAuAPgTvOYuwuYBcA157XZUk6DzquUE4A1yzav3p47C0XAz8PfDPJPwE3Avun3Zitqr1VNamqCVc0rEzSquoIytPA5iTXJVkP3Absf+tgVb1WVZdX1aaq2gQcBLZX1aGGuSWNyMxBqao3gbuBx4AXgK9U1fNJ7kmyfdbXl7R2pKrmvYapMknhNcyaVZn3CjSLwOGqWvHPi/mTspLaGBRJbQyKpDYGRVIbgyKpjUGR1MagSGpjUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTGoEhqY1AktTEoktqM9ze2Jf8OHJn3Os6jy4EfzHsR55Hnt7b9bFVdvNInnfc/ozGDI+fyK+jWiiSHPL+16//D+Z3L83zLI6mNQZHUZsxB2TvvBZxnnt/a5vlNMdqbspLWnjFfoUhaY0YTlCSXJnk8ydHh8yXLjPuvJM8MH/unjRmTJDclOZLkWJLdU45flOSh4fhTSTat/irP3Vmc3x1Jvr/oa/Yb81jnuUjyhSSnknxnmeNJ8ifDuT+b5IbVXuMszuL8PprktUVfu98944tW1Sg+gM8Bu4ft3cB9y4x7fd5rXcE5XQi8CFwPrAe+DWxZMuYu4M+G7duAh+a97ubzuwP4/LzXeo7n9xHgBuA7yxy/BXgUCHAj8NS819x8fh8F/mYlrzmaKxRgB7Bv2N4H3DrHtXTZChyrqpeq6g3gQRbOc7HF5/0w8LEka+UPeZ7N+a1ZVfUt4EenGbID+GItOAi8J8nG1Vnd7M7i/FZsTEHZUFUnh+1XgA3LjHtHkkNJDiYZe3SuAl5etH98eGzqmFr4w/OvAZetyupmdzbnB/CJ4S3Bw0muWZ2lrYqzPf+17ENJvp3k0STvO9PgVf1J2SRPAFdOObRn8U5VVZLlvv303qo6keR64Mkkz1XVi91rVZu/Br5cVT9J8pssXI39ypzXpLPzDyz89/Z6kluAvwI2n+4JqxqUqtq23LEkrybZWFUnh8vGU8u8xonh80tJvgl8gIX38WN0Alj8L/LVw2PTxhxPsg54N/DD1VnezM54flW1+FweYOFe2dvF2Xx916yq+rdF219P8qdJLq+qZf8fpjG95dkP7By2dwKPLB2Q5JIkFw3blwO/CHx31Va4ck8Dm5Ncl2Q9Czddl35navF5fxJ4soY7YmvAGc9vyT2F7cALq7i+820/8Onhuz03Aq8tetu+5iW58q37eUm2stCL0/9jN+87zYvuKF8GHACOAk8Alw6PT4AHhu0PA8+x8N2E54A7573uszivW4DvsXAVtWd47B5g+7D9DuCrwDHg74Hr573m5vP7PeD54Wv2DeDn5r3mFZzbl4GTwH+ycH/kTuAzwGeG4wHuH879OWAy7zU3n9/di752B4EPn+k1/UlZSW3G9JZH0hpnUCS1MSiS2hgUSW0MiqQ2BkVSG4MiqY1BkdTmfwDGux0z5v/0/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_image = np.reshape(img_x, (1, 4, 4, 3))\n",
    "# predicted_image = np.reshape(output_cl1, (1, 4, 4, 3))\n",
    "visual_image = np.reshape(patch.eval(session=sess2), (1, 1, 2, 3))\n",
    "visual_label = np.reshape(lbl_x, (1, 4, 4, 3))\n",
    "\n",
    "# visual_image_conn = np.reshape(output_fully_connected, (1, 120, 120, 3))\n",
    "# visual_label_conn = np.reshape(lbl_x, (1, 120, 120, 3))\n",
    "see_output(original_image)\n",
    "see_output(visual_label)\n",
    "# see_output(predicted_image)\n",
    "see_output(visual_image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
